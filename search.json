[{"categories":["Programming","Java"],"content":"` 상속과 다형성에 대해 알아보자\nInheritance B가 A의 member variables과 method를 그대로 받으면 상속받는다고 하고 부모-자식, 상위-하위 관계이다.\n 기존의 클래스에서 자산(변수,메서드)을 자식 클래스에서 재사용 → 코드의 절감 접근 제한자에 상관없이 상속되지만 자식에게 보이지 않을 뿐.. 어떤 Class가 아무런 상속을 받지 않을 경우, 자동으로 java.lang.Object.Class가 Class의 부모가 된다.  1 2 3  public class B extends A{ ... }   자바는 다중 상속 불가능\nis a\n extends 관계  has a\n 상속하지 못한다고 버릴 필요는 없고 멤버 변수로 가지고 있기  구분..?\n is a 적합한지 보기 (ex superman is a person? superman is a super?) 나머지는 has a  super : 조상 class의 생성자 호출\n super(name) 처럼 생성자로 넘기는 식으로 활용 가능 this를 통해서 가지고 있는 것과 상속받은 것을 확인할 수 있다.  this는 나의 다른 생성자를 부를 때도 사용\n주의사항\n  둘 다 첫 줄에 사용해야 함\n→ this와 super를 같이 쓸 순 없음\n  다음은 가능\n1 2 3 4  public Corona(String name, int level, int spreadSpeed){ super(name,level); this.spreadSpeed = spreadSpeed; }     명시적으로 this 또는 super를 호출하지 않으면 생성자의 첫 줄에는 super() 가 생략되어 있음.\n  하위 class에서 private, default로 접근할 수 없기 때문에 접근해야 할 것은 protected사용하거나 public method활용\n      상속      Object toString   Virus -   Corona toString   ChildCorona -    ","description":"","tags":null,"title":"OOP3","uri":"/posts/programming/java/oop3/"},{"categories":["Programming","Java"],"content":"` 상속과 다형성에 대해 알아보자\nInheritance B가 A의 member variables과 method를 그대로 받으면 상속받는다고 하고 부모-자식, 상위-하위 관계이다.\n 기존의 클래스에서 자산(변수,메서드)을 자식 클래스에서 재사용 → 코드의 절감 접근 제한자에 상관없이 상속되지만 자식에게 보이지 않을 뿐.. 어떤 Class가 아무런 상속을 받지 않을 경우, 자동으로 java.lang.Object.Class가 Class의 부모가 된다.  1 2 3  public class B extends A{ ... }   자바는 다중 상속 불가능\nis a\n extends 관계  has a\n 상속하지 못한다고 버릴 필요는 없고 멤버 변수로 가지고 있기  구분..?\n is a 적합한지 보기 (ex superman is a person? superman is a super?) 나머지는 has a  super : 조상 class의 생성자 호출\n super(name) 처럼 생성자로 넘기는 식으로 활용 가능 this를 통해서 가지고 있는 것과 상속받은 것을 확인할 수 있다.  this는 나의 다른 생성자를 부를 때도 사용\n주의사항\n  둘 다 첫 줄에 사용해야 함\n→ this와 super를 같이 쓸 순 없음\n  다음은 가능\n1 2 3 4  public Corona(String name, int level, int spreadSpeed){ super(name,level); this.spreadSpeed = spreadSpeed; }     명시적으로 this 또는 super를 호출하지 않으면 생성자의 첫 줄에는 super() 가 생략되어 있음.\n  하위 class에서 private, default로 접근할 수 없기 때문에 접근해야 할 것은 protected사용하거나 public method활용\n      상속      Object toString   Virus -   Corona toString   ChildCorona -    ","description":"","tags":null,"title":"OOP2","uri":"/posts/programming/java/oop2/"},{"categories":["Algorithm"],"content":"정렬 ` 정렬해 보았다.\n 람다식 Arrays.sort는 2차원 배열은 정렬할 수 없다. 이를 람다식을 이용해 해결할 수 있다.\n예시로 좌표 정렬하기를 풀어보았고 아래는 정렬 부분 코드이다.\n1 2 3 4 5 6  Arrays.sort(arr1, (e1, e2) -\u003e{ if(e1[0]==e2[0]) return e1[1] - e2[1]; else return e1[0] - e2[0]; });   다음은 단어 정렬에 사용한 코드이다.\n1 2 3 4 5 6 7 8 9  Arrays.sort(arr1, new Comparator\u003cString\u003e() { public int compare(String s1, String s2) { if (s1.length() == s2.length()) { return s1.compareTo(s2); } else { return s1.length() - s2.length(); } } });   Comparator 에서 에 있는 것은 상속관계에 있는 타입까지 허용한다는 뜻인데 여기서는 T자체만 봐도 상관없다. 또한 method를 정의하여 사용할 수 있다.\n이렇게 람다식을 사용해 간결한 표현을 할 수 있다. python의 lambda와 함수와 비슷하여 어떻게 돌아가는 이해는 했으나 사용방식이 익숙치 않아 좀 더 연습해봐야 할 것 같다.\n","description":"","tags":null,"title":"정렬, 람다식","uri":"/posts/algorithm/%EC%A0%95%EB%A0%AC-%EB%9E%8C%EB%8B%A4%EC%8B%9D/"},{"categories":["Algorithm"],"content":"` 자바에서 입출력에 관련해서 다뤄보겠다. 기본적으로 사용한 코드의 문제점은 다음과 같다.\n 입력이 많을 경우, Scanner의 문제 출력이 많을 경우 ,System.out의 문제  몇몇 문제의 경우 위의 문제가 해결되지 않을 경우 시간초과가 난다고 하여 BufferedReader, StringBuilder 등을 알게 되어 사용법을 공부하였다.\nBufferedReader 버퍼를 사용하여 입력을 받고 한 번에 전송하는 방식을 사용하여 하나씩 전송하는 Scanner보다 효과적임.\n 출력으로 BufferedWriter가 있고 추후에 사용해볼 예정.  StringBuilder System.out 의 잦은 사용 및 String과 String을 더하고 빼는 과정에서 성능이 좋지 않음\n append를 이용해 이어붙여서 출력  아래 코드를 기본으로 사용한다.\n1 2  BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); StringBuilder sb = new StringBuilder();   이를 이용해 다음 문제를 풀어보았다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import java.io.BufferedReader; import java.io.InputStreamReader; public class a { public static void main(String[] args) throws Exception { BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); int N = Integer.parseInt(br.readLine()); StringBuilder sb = new StringBuilder(); for (int i = 0; i \u003c N; i++) {\tString str = br.readLine(); int target = str.indexOf(\" \"); int result = Integer.parseInt(str.substring(0,target)) + Integer.parseInt(str.substring(target + 1));\tsb.append(result+\"\\n\"); } br.close(); System.out.print(sb); } }   추가로 StringTokenizer가 있고 추후 공부할 예정이다.\n","description":"","tags":null,"title":"[백준]_15552_빠른A+B(입출력 문제)","uri":"/posts/algorithm/%EB%B0%B1%EC%A4%80_15552_%EB%B9%A0%EB%A5%B8a+b/"},{"categories":["Programming","Java"],"content":"OOP ` 우리는 객체 지향적 삶을 살고 있고 그러한 현실을 프로그래밍에 반영하려고 함.\n장점\n 객체 교체(유지 보수)에 좋음 재사용성  OOP의 특징 OOP is A P.I.E\n  Abstraction : 특징 추출\n→ 현실의 객체(프로그램의 대상으로 삼는 것)를 추상화해서 class를 만들고 이를 구체화해서 object를 만든다.\n  Encpsulation : 필요한 기능 공개\n→ 데이터의 은닉과 보호\n  inheritance : 상속\n→ member variables, method를 자식이 물려받음 → 재사용\n  Polymorphism : 다형성\n→ 하나의 객체를 여러 타입으로 참조하는 것\n  Class class : 현실 세계를 추상화 해놓은 것 ex) 설계도, 청사진 등\n→ object를 만들기 위해 필요하지만 직접 class를 사용하진 않음.\n객체(object,instance) : 클래스를 구체화해서 메모리에 생성된 것\nclass = type\nfactor\n attribute : member variables → 각각 객체마다 다를 수 있음 behavior : methods → 동작, 객체들마다 같음 Constructor(생성자) → member variable 초기화  new\nConstructor를 보고 memory allocation 수행\n 생성자는 기본부터 여러 parameter를 가지는 등 다양하게 생성 가능  member variables\n 다양한 상태 표현 설정하지 않으면 default value 부여 OOP적 관점에서 외부에서 값을 바꾸는 것은 좋지 않음   Encapsulation   member variables과 method를 필요한 경우를 제외하고 노출하지 않도록 가정\n  노출할 경우 set\u0026get 으로 소통\n  여전히 외부에서 접근가능하므로 private 설정\n  new는 heap 영역에 객체를 생성한다는 의미!\n  1 2 3 4 5 6 7 8 9 10 11 12 13  int i1 = 10; int i2 = 10; String s1 = \"Hello\"; String s2 = \"Hello\"; // s1과 s2는 같은 곳을 가리킴 String s3 = new String(\"Hello\"); String s4 = new String(\"Hello\"); //new를 통해 생성하면 따로 생성됨  // == 는 메모리값 비교함 if( i1 == i2 ) { System.out.println(\"i1 i2 Same\"); } if( s1 == s2 ) { System.out.println(\"s1 s2 Same\"); } if( s3 == s4 ) { System.out.println(\"s3 s4 Same\"); } //equals 쓰면 나옴    String class StringBuilder\n+를 사용하면 불필요한 객체가 많아져 사용\nloop 등에서는 stringBuilder가 효과적\nappend를 이용\n1 2  StringBuilder sb = new StringBuilder(\"\"); sb.append(s1).append(\", \").append(s2);   toString 사용\n toString이 자동으로 생성되고 재정의해서 사용  1 2 3  public String toString() { return this.name + \" \" + this.color + \" \" + this.price; }   1 2  // main System.out.println(phone); // Galaxy Note B 10000   값 전달\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  public class PassByValueTest { public static void main(String[] args) { int i = 10; setVal(i); // 값 전달,최종적으로 안 바뀜. \tSystem.out.println(i); Pass p = new Pass(); p.val = 10; setVal(p); // reference 전달, 주소값을 찾아 값이 5로 바뀜. \tSystem.out.println(p.val); } public static void setVal(int x) { x = 5; } public static void setVal(Pass p) { p.val = 5; } } class Pass{ public int val = 3; }   Package  같은 패키지일 경우 package 선언하면 사용가능 다른 패키지일 경우 import로 불러와야 함. (com.web.*)  Access Modifier    구분 Same Class Same Package Sub Class Universe     private O X X X   default O O X X   protected O O O X   public O O O O    ","description":"","tags":null,"title":"OOP1","uri":"/posts/programming/java/oop1/"},{"categories":["Programming","Java"],"content":"배열 for each Array 1 2 3 4 5  int arr [] = {1,2,3,4,5}; for(int x : arr){ System.out.println(x); }   Array is immutable  크기 변경 불가 변경이 필요할 경우 새로 작성  arraycopy\n1 2 3 4  String [] students = { \"홍길동\", \"박사\", \"윤식당\", \"나오기\" }; String [] students3 = new String[5]; System.arraycopy(students, 0, students3, 0, 4); //[홍길동, 박사, 윤식당, 나오기, null] // index와 length를 정해 copy   1 2 3 4  int[] srcArray = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}; int[] tgtArray = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}; System.arraycopy(srcArray, 2, tgtArray, 1, 3); // [0, 2, 3, 4, 0, 0, 0, 0, 0, 0]   1 2 3 4 5 6 7 8 9  int[] intArray = { 3, 27, 13, 8, 235, 7, 22, 9, 435, 31, 54 }; int min = Integer.MAX_VALUE; int max = Integer.MIN_VALUE; for (int i = 0; i \u003c intArray.length; i++) { min = Math.min(min, intArray[i]); max = Math.max(max, intArray[i]); }   valuecount\n1 2 3 4 5 6 7 8  int[] intArray = { 3, 7, 2, 5, 7, 7, 9, 2, 8, 1, 1, 5, 3 }; int[] used = new int[10]; for (int i = 0; i \u003c intArray.length; i++) { used[intArray[i]]++; }   2차원 배열 array 만들기\n int [][]Array = new int[4][3]; int [][] intArray5 = new int[][] {{1,2,3},{1,2,3},{1,2,3},{1,2,3}};  1 2 3  int intArray[][] = new int [4][]; intArray[0] = new int[2]; intArray[2] = {1,2,3}; // 안됨    InputStreamReader와 BufferedReader를 이용해 입력처리를 빠르게 할 수 있음.   Array memory 1 2 3 4 5 6 7 8 9 10  static void makeAndPrint() { // 로컬 영역 \tint [] arr1 = new int[3]; int [] arr2 = {1,2,3} // array constant  // arr2 = {4,5,6} 불가능 \tarr2 = new int[] {4,5,6}; // 기존의 공간이 사라지고 새로운 공간이 할당됨. // 기존의 공간은 누구도 참조하지 않고 GC가 자동으로 회수 }   ` stack → local　heap → 객체\n int [] arr1 : arr1 (참조형) 생성 new int[3] : 3개의 int를 저장할 공간 생성(heap에 만들기!)  new로 생성할 경우 얼마나 공간을 차지할지 알려줘야 함.   32bit*3의 공간의 주소가 stack에 저장된다.  자바는 GC가 자동으로 사용하지 않는 메모리를 회수함\nn차원 배열  2차원  1차원 배열을 관리하는 1차원 배열이라 할 수 있음\n  int [][] arr3 = new int[3][]\n arr3의 3개의 공간에 각각 int[]가 들어가야 함.(기본값 null)  1 2 3  arr3[0] = new int[]{1,2,3,4,5}; arr3[1] = new int[]{6,7}; arr3[2] = new int[]{8,9};   Reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  static void reference() { char [] chars = \"Hello\".toCharArray(); change1(chars[0]); // 값이 복사되어 change로 넘어감. \tSystem.out.println(Arrays.toString(chars)); // 실제 H는 바뀌지 않았음 \tchange2(chars); // chars의 주소값을 넘겼기 때문에 바뀌게 된다. } static void change1(char data) { System.out.println(data+32); // 104 \tdata+=32; // 단항 연산자에서는 형변환이 일어나지 않음. \tSystem.out.println(data); // h } static void change2(char[] c2) { c2[0]+=32; }   엄밀히 말하면 위에서 chars는 배열이기 보다는 배열을 가리키는 주소값이라 할 수 있음\nswap\n1 2 3 4 5 6 7 8 9 10  static void swapTest() { int[] arr = {1,2,3}; swap(arr,1,2); // arr의 주소가 넘겨짐 } static void swap(int[] arr ,int a, int b) { int t = arr[a]; arr[a] = arr[b]; arr[b] = t; // 실제로 값이 바뀜. }   Arrays class  배열을 사용할 때 유용한 기능 제공  배열의 제약사항\n 타입, 크기  1 2 3 4 5 6 7 8 9 10 11 12  char [] chars = \"Hello Ssafy 5th class 12!!\".toCharArray(); char [] largeOne = Arrays.copyOf(chars, 40); // 한꺼번에 복사  char [] copy2 = Arrays.copyOfRange(chars, 0, 5); // 일부분 복사  Arrays.fill(largeOne, '#'); // 특정 값으로 채우기, 초기화에 유용 \tArrays.sort(chars); // 정렬    Array Delta Traversal 여태까지는 indexing을 이용한 방법\n방향을 나타내는 delta 행렬 선언\n1 2 3 4  static int[][] deltas = { { -1, 0 }, { 0, 1 }, { 1, 0 }, { 0, -1 } }; // 상하좌우를 나타내는 Delta 배열 static int[][] deltaPlus = { { -1, -1 }, { -1, 1 }, { 1, 1 }, { 1, -1 } }; // 대각선   1 2 3 4 5 6 7 8 9 10 11 12  for(int r=0;r\u003c3;r++) { for(int c=0;c\u003c3;c++) { int sum = 0; for(int d=0;d\u003c4;d++) { int nr = r + deltaPlus[d][0]; int nc = c + deltaPlus[d][1]; if(isIn(nr,nc)) sum +=map[nr][nc]; } result[r][c] = sum; } }   ","description":"","tags":null,"title":"Java_Programming2","uri":"/posts/programming/java/java_programming2/"},{"categories":["Algorithm"],"content":"`위 문제 유형은 Dynamic Programming이다. 규칙을 찾아서 적용하면 되겠다.\n과정   선언\n 2*n 의 최대값이 246912이므로 246913의 소수 여부 배열(boolean) 위와 같은 크기의 int배열 선언하여 1부터 소수가 몇 개 있는지 저장    값 할당\n 에라토스테네스 체 원리 이용하여 소수 여부를 true로 바꿈 2부터 반복문을 이용해 false가 나올 때마다 count를 올려주는 식으로 코드를 구성    최종 풀이\n 수를 입력받아 2*n까지의 소수 개수에서 n개까지 소수 개수를 빼고 정답 출력     이 문제는 python으로 풀었음.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import sys input = sys.stdin.readline N = int(input()) dp = [0 for _ in range(N+3)] arr = [0 for _ in range(N+3)] for k in range(1,N+1): arr[k] = int(input()) dp [1] = arr[1] dp [2] = arr[1] + arr[2] dp [3] = max(arr[1] + arr[3] ,arr[2] + arr[3]) for i in range(4, N+1): dp[i] = max( dp[i-3] + arr[i-1] + arr[i] , dp[i-2] + arr[i] ) print(dp[N])   ","description":"","tags":null,"title":"[백준]_2579_계단오르기","uri":"/posts/algorithm/%EB%B0%B1%EC%A4%80_2579_%EA%B3%84%EB%8B%A8%EC%98%A4%EB%A5%B4%EA%B8%B0/"},{"categories":["Algorithm"],"content":"위의 문제를 풀기 위해 에라토스테네스의 체의 원리를 이용했다. 에라토스테네스의 체는 소수를 구하기 위한 알고리즘 중 가장 성능이 좋은 방법으로, 소수의 배수를 거름으로써 건너뛰는 작업이 많아진다.\n과정   선언\n 2*n 의 최대값이 246912이므로 246913의 소수 여부 배열(boolean) 위와 같은 크기의 int배열 선언하여 1부터 소수가 몇 개 있는지 저장    값 할당\n 에라토스테네스 체 원리 이용하여 소수 여부를 true로 바꿈 2부터 반복문을 이용해 false가 나올 때마다 count를 올려주는 식으로 코드를 구성    최종 풀이\n 수를 입력받아 2*n까지의 소수 개수에서 n개까지 소수 개수를 빼고 정답 출력     전체 코드는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  import java.util.Scanner; public class Test { public static void main(String[] args) { Scanner sc = new Scanner(System.in); boolean[] check = new boolean[246913]; int[] arr = new int[246913]; check[0] = check[1] = true; for(int i=2; i\u003c=Math.sqrt(246913);i++) { if(check[i]==true) continue; for(int j=i*i; j\u003c=246913; j=j+i) check[j]=true; } int count =0; for(int i=2;i\u003c246913;i++) { if(!check[i]) count++; arr[i] = count; } int n = 1; while(true) { n = sc.nextInt(); if (n==0) break; System.out.println(arr[2*n] - arr[n]);\t} } }   ","description":"","tags":null,"title":"[백준]_4948_베르트랑공준","uri":"/posts/algorithm/%EB%B0%B1%EC%A4%80_4948_%EB%B2%A0%EB%A5%B4%ED%8A%B8%EB%9E%91%EA%B3%B5%EC%A4%80/"},{"categories":["Programming","Java"],"content":"Java Basic 환경\n jdk : 소프트웨어 개발 jre : 실행 환경  다음 파일(HelloWorld.java)를 커멘드 상 실행하기 1 2 3 4 5 6 7  package com.ss.java_basic1 public class HelloWorld { public static void main(String[] args){ System.out.println(\"Hello World\"); } }    javac -d . HelloWorld.java 로 class 생성 java 클래스 이름으로 실행  Type\n primitive : 정해진 크기의 memory size reference : 정해질 수 없음, 공간의 주소를 저장  Type casting\n 묵시적 : 큰 type → 작은 type  정수형은 실수형으로 자동형변환   명시적 : 작은 type → 큰 type  /** 코드 */ : Javadoc 사용\nbit operator\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  public class Test { public static void main(String[] args) { // bit and \tSystem.out.println(\"3 \u0026 4 = \" + (3 \u0026 4)); // 0011 \t// \u0026 0100 \t// ------ \t// 0000 \t// bit or \tSystem.out.println(\"3 | 4 = \" + (3 | 4)); // 0011 \t// | 0100 \t// ------ \t// 0111 \t// bit exclusive(xor) \tSystem.out.println(\"3 ^ 4 = \" + (3 ^ 4)); // 0011 \t// ^ 0100 \t// ------ \t// 0111 \t// bit not \tSystem.out.println(\" ~ 4 = \" + (~ 4)); // 0100 \t// ~ \t// ------ \t// 1011 \t} }   Bit 연산  *,/ 연산자에 비해 처리 속도가 훨씬 빠름.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  public static void main(String[] args) { // \u003c\u003c \tSystem.out.println(\"1 \u003c\u003c 2 = \" + (1 \u003c\u003c 2)); // .... 0000 0001 \t// \u003c\u003c .... 0000 0100 1 \u003c\u003c 2 = 4 \tSystem.out.println(\"-16 \u003e\u003e 2 = \" + (-16 \u003e\u003e 2)); //\t.... 1111 0000 \t// \u003e\u003e .... 1111 1100 -16 \u003e\u003e 2 = -4 \t// \u003e\u003e\u003e \tSystem.out.println(\"7 \u003e\u003e\u003e 2 = \" + (7 \u003e\u003e\u003e 2)); //\t0000 .... 0000 0111 \t// \u003e\u003e\u003e 0000 .... 0000 0001 7 \u003e\u003e\u003e 2 = 1 \tSystem.out.println(\"-5 \u003e\u003e\u003e 24 = \" + (-5 \u003e\u003e\u003e 24)); //\t1111 1111 1111 1111 1111 1111 1111 1001 \t// \u003e\u003e\u003e 0000 0000 0000 0000 0000 0000 1111 1111  // -5 \u003e\u003e\u003e 24 = -1 \t}   조건문\n switch에서는 double 사용x ( String은 Java 버전 바뀜에 따라 됨) break 없으면 계속 내려가면서 검사 ‘A’와 65 둘 다 case에 있으면 겹치기 때문에 오류남.  삼항 연산\n1 2 3 4 5 6 7 8 9 10  public static void main(String[] args) { int N = 6; boolean isEven = ( N % 2 == 0 ) ? true : false; N = ( ! isEven ) ? 10 : 20; System.out.println(N); // 20 \t}   데이터 타입 문제 정수의 문제점 : 범위 문제(Overflow)\n1 2 3 4 5 6 7 8 9 10  int i = Integer.MAX_VALUE; // 가장 큰 수 알아보기 int i2 = i + 1; System.out.println(i); // 2147483647 System.out.println(i2); // -2147483648 , Integer.MIN_VALUE 가 되버림.  long l1 = i+1; long l2 = (long)(i+1); // 이미 깨진걸 사용해서 -2147483648가 나옴 \tlong l3 = (long)i + 1; // 2147483648, 형 변환 후 연산   ` 그렇지만 long도 무한대는 아니라 한계 존재…\n→ 별도의 class(BigInteger 등)로 표현!  실수의 문제점 : 부동소수점\n  컴퓨터는 정확한 실수를 표현하지 못함(근사값 사용)\n→ 실수의 계산은 믿을만한 게 아님(오차 허용)\n→ 정수로 올려서 계산하면 된다.\n  BigDecimal등을 이용할 수 있으나 무거움.\n값의 동등 비교  기본형 : == 객체형 : equals method  ","description":"","tags":null,"title":"Java_Programming1","uri":"/posts/programming/java/java_programming1/"},{"categories":null,"content":"관심 분야 : 데이터 분석, 빅데이터, AI\nCareer  2019 Samsung SDS Brightics Contest 참가(2019.07 ~ 2020.08) L point Competetion(2019.12 ~ 2020.) Dacon 참여  반도체 박막 두께 분석(16등) 천체 분류 대회 온도 추정 대회   빅데이터 청년 인재 고려대학교 과정(지능 정보 시스템) 수료 (2020.07 ~ 2020.09) AI Innovation 고급 과정 시각반(2020.09 ~ 2020.10)  Study  기계학습 스터디(2019.03~2019.06)  공유 자료로 공부 후 미세먼지량 예측   머신러닝, 딥러닝 스터디(2019.08~2020.03)  밑바닥부터 시작하는 딥러닝, 모두를 위한 딥러닝(tensorflow), Hands on Machine Learning   online kaggle study(2020.03~2020.05) ADP 실기 study (2020.09~) Algorithm(2020.01~)  Course 들었던 인터넷 강의 모음\n edwith  python, R , 선행대수학 Statistics 110 Machine Learning   Fast Campus 데이터 분석 K-mooc 텍스트 마이닝 Coursera  TensorFlow for Artificial Intelligence Data Science: Statistics and Machine Learning 특화 과정   T-academy  Git, Github page audio 데이터 처리    certificate  정보처리산업기사(2018.11) ADsP(2019.12) ADP(필기만, 2020.07)  Stack(언어/기술)  Python R C, C++ SQL HTML/CSS/JS Flask Git Data analysis Machine Learning Deep learning Computer Vision  Site Github  https://github.com/GyuYoungCho https://gyuyoungcho.github.io  Kaggle  https://www.kaggle.com/rbud613  Assignment  Bayesian 분류기 구현 Mnist 손글씨 인식 Cartpole 딥러닝 강화 학습  Project  잠재 고객 상품 추천 오디오 분류 식생활을 위한 음식 추천 CCTV를 이용한 날씨 분류(https://github.com/GyuYoungCho/Capstone)  ","description":"","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"HI! 이사했어 ","description":"","tags":null,"title":"Test","uri":"/posts/test/"},{"categories":["Certificate"],"content":"데이터 분석 R기초와 데이터 마트 R언어와 문법 데이터 구조  벡터 : 하나의 스칼라값, 하나 이상의 스칼라 원소(동일한 자료형) 행렬 -\u003e matrix 데이터 프레임 -\u003e data.frame(incom=a1,car=b1) 배열 -\u003e array(1:12, dim=c(3,4)) 리스트 : list(name=“a”,height=123)  기초 함수  solve : 역행렬 cov, cor : 공분산, 상관계수 substr(char,num1,num2) : num1번째에서 num2번째 계산 format(sys.Date(),'%a') : 현재 요일 출력   pairs : 산점도 행렬 melt(data,id) : id기준으로 variable, value 저장 cast(data,day~month~variable) : day, month기준으로 변수 배열 acast(data,month~variable,mean) : month 당 평균 subset 등으로 특정 변수만 선택가능 sqldf(“쿼리문”)  결측치 처리와 이상값 검색 결측값 대치법  평균 대치법 : 적절한 평균값으로 결측값 대치 단순확률 대치법 : 평균대치법에서 표준오차의 과소추정 문제를 보완, 적절한 확률값 부여 후 대치 다중 대치법 : 통계량 효율성 및 일치성 문제 보완, 과소 추정 문제는 여전   complete.cases() : 행의 모든 값이 na아닌 경우 True  이상값 : boxplot 보기\n통계분석 통계학개론 확률적 추출\n 단순 무작위 추출 : 임의로 개수 뽑기 계통추출 : 일정 간격으로 표본 추출 층화추출 : 집단, 층으로 나누고 집단 내 원하는 크기의 표본 무작위 추출 군집추출 : 집단을 나눠 집단 선택해 표본 추출  비확률적 추출\n 판단추출 : 연구자의 판단으로 추출 할당추출 : 여러 집단으로 나눠 각 집단에서 연구자의 판단으로 추출 편의추출 : 쉽게 접근할 수 있는 표본 추출  자료 : 명목 서열 등간 비율\n좋은 추정량 조건 불편성, 효율성, 충족성, 일관성\n검정 방법  모수적 검정   가정된 분포의 모수에 대해 가설 설정 표본평균, 분산 등을 구해 검정을 실시  비모수적 검정   분포에 제약을 가정x, 특정 분포를 따른다고 가정할 수 없는 경우 가설은 단지 분포의 형태가 동일하다 아니다로 분포의 형태에 대해 설정 순위나 부호 등을 이용해 검정 부호검정,윌콕슨, 부호순위합, 스피어만 순위상관, 만 위트니  회귀분석 가정\n 선형성 독립성 등분산성 비상관성 정상성(잔차항이 정규분포)   Residuals vs Fitted는 y축의 잔차를 보여줌, 기울기가 0인 직선이 이상적 noraml Q-Q는 잔차가 정규분포를 따르는지 확인, 직선 상에 있어야함 Scale Location은 y축의 표준화 잔차를 나타냄, 기울기가 0인 직선이 이상적 Residuals vs Leverage에서 cook’s distance가 1이 넘어가면 영향점이라 판단  다중공선성  변수들이 상관되어 있을 때 발생, vif가 4가 넘으면 다중공선성이 존재한다고 봄  최적으 회귀방정식 선택  aic, bic로 적합성을 봄 단계적 변수선택법  1  step(lm(y~1,df),scope=list(lower=~1,upper=~x1+x2),direction=\"forward\")   릿지회귀 : l2 norm 최소화 라쏘회귀 : l1 norm 최소화 엘라스틱넷 : 위의 두 개 절충\n다변량 분석  상관분석 서열척도 - 스피어만 상관분석 등간, 비율척도 - 피어슨 상관, 편상관분석  cor.test() : 상관계수 검정\n다차원 척도법(MDS)   유사성을 측정해 2, 3차원 공간에 표현  주성분 분석  차원 축소 기법 biplot : 2개의 주성분만 2차원의 그래프로 표현  시계열 분석  정상성   분산과 수준에 체계적 변화가 없고 주기적 변동이 없음, 미래는 확률적으로 과거와 동일  비정상성시계열을 정상성으로 만들기   평균이 일정하지 않은 경우 차분을 하면 됨 계절성을 가질 경우 계절차분 사용 분산이 일정하지 않은 경우 로그변환 등 진행  백색잡음 과정 : 평균이 0, 분산이 일정, 자기공분산 0 자기상관 : t, t-1간 상관 관계\n시게열 모형  자기회귀모형(AR) : 현시점의 자료에 몇 번째 전 자료까지 영향을 주는가   AR(1) 모형은 백색잡음값과 1스텝 과거의 자기 자신의 값만의 가중합임 자기상관함수 : 점차적 감소 부분자기상관함수 : 급격히 감소해 절단  이동평균모형(MA)   데이터의 평균을 예측치로 사용, 동일한 가중치 항성 정상성 만족 자기상관함수 : 급격히 감소해 절단  자기회귀누적이동 모형(ARIMA)   차분과 변환을 통해 정상화 ARIMA(p,d,q)에서  d=0 -\u003e ARMA(p,q) p는 AR, q는 MA와 관련   두 개의 상관함수가 지수적 감소  분해 시계열   추세요인 : 특정한 형태 계절요인 : 고정된 주기에 따라 변화 순환요인 : 알려지지 않은 주기를 가지고 자료가 변화 불규칙요인 : 설명할 수 없는 요인(오차)  1 2 3 4 5 6 7 8 9  diff(data,differences=1) # 한번 차분 decompose(data) # 시계열을 4가지 요인으로 분해 acf(data,lag.max=20) pacf(data,lag.max=20) # 자기상관함수와 부분자기상관함수 auto.arima(data) # 적절한 ARIMA모형 결정   정형 데이터 마이닝 분류분석 로지스틱 회귀  오즈비 : 성공율/실패율     구분 선형 회귀분석 로지스틱 회귀분석     종속변수 연속형 이산형   모형 탐색 방법 최소자승법 최대우도법, 가중최소자승법   모형 검정 F-test, t-test 카이제곱 test    로짓변환 : log(p/(1-p))\n최대우도 추정법\n1  glm(pmale~x,family='binomial',weight=total)   estimate값의 exp값이 오즈의 배수가 됨\n신경망 모형 활성함수 : 시그모이드, 부호, 소프트맥스\n의사결정나무 부모마디 : 상위마디 자식마디 : 하위마디 뿌리마디 : 맨 위의 마디 최종마디 : 더 이상 분할되지 않음\n분류나무  지니지수 : 1-p^2의 합으로 계산 엔트로피 지수 : -plogp의 합으로 계산  회귀나무  F통계량의 p값, 분산감소량이 분류 기준값의 선택 방법이 됨  기준변수의 선택법\n   구분 이산형 연속형     CHAID 카이제곱통계량 ANOVA 통계량   CART 지니 지수 분산 감소량   C4.5 엔트로피 지수 X     장점  해석이 용이. 수학적 가정이 불필요한 비모수적 모형 계산 비용 낮음, 상호작용 및 비선형성 고려가능   단점  경계선 부근에서 오차가 큼 각 예측변수의 효과를 파악하기 어려움    양상블 모형   배깅 : 임의 복원추출해 각 표본에 대해 분류기를 생성 후 결과를 앙상블\n  부스팅 : 동일한 확률이 아닌 분류가 잘못된 데이터에 가중치를 주어 표본 추출\n  랜덤 포레스트 : 배깅 + 랜덤\n  SVM  데이터 간 간격이 최대가 되는 선을 찾아 기준으로 데이터 분류 장점  에러율 낮고 결과 해석 용이   단점  파라미터 선택에 민감, 이진 분류만 가능   COST로 비용의 합 최소화하는 선 찾음  나이브 베이즈  사후확률은 사전확률을 통해 예측할 수 있다. 장점  TRAINING 데이터가 적어도 사용가능 multi-class를 쉽고 빠르게 예측   단점  한 쪽에 없는 경우 정상적 예측 불가 확률적으로 독립이라는 가정이 위반되면 오류 발생    모형평가 홀드아웃  train과 test 나눠서 실행  교차 검증  K개로 나눠 K-1를 훈현, 1개를 테스트로 하여 K번 반복  붓스트랩   오분류표 : 분류에서 사용\n  ROC그래프\n   X축에는 FP Ratio(1-특이도), y축에는 민감도를 나타냄 면적이 넓을 수록 좋은 모형  이익도표와 향상도   이익 : 개체들이 각 등급에 얼마나 분포하는가, 이익값을 누적으로 연결하면 이익도표 향상도 곡선 : 모델의 성과가 얼마나 향상되었나  군집분석  계층적 군짐   단일 연결법 : 짧은 거리로 고립된 군집을 찾음 완전 연결법 : 거리의 최댓값, 내부 응집성 중심 평균연결법 : 거리 평균으로 군집화, 계산량이 많음 중심연결법 : 중심간 거리, 가중 평균 와드연결법 : 오차제곱합 기초  거리 측정 유클리드 : 거리 제곱합의 제곱근 맨하튼 : 절대값의 합 민코프스키 거리 : q=2면 유클리드, q=1이면 맨하튼\n마할라노비스 : 표준화와 상관성을 고려한 통계적 거리\n비계층적 군집 k-mean  k개 임의 선택 군집의 중심점으로부터 오차제곱합이 최소가 되로록 각 자료 할당 군집 내 평균을 계산해 군집 중심 갱신 군집 중심 변화가 없을 때까지 2,3 반복     장점  단순, 많은 양 처리 사전적 정보 없이 의미있는 자료 분석 가능   단점  k값을 정해야 함 잡음이나 이상값에 영향받기 쉬움(평균 대신 중앙값을 사용하는 k-medoids 사용)    혼합분포 군집    EM 알고리즘 : 혼합분포에서 잠재변수를 추정할 때 사용\n K개의 클러스터 초기화 포인터가 클러스터에 포함될 확률 계산 MSL이 최대화 되기 위한 파라미터 계산    kmean은 중심거리 EM은 MSL로 거리를 측정\n  SOM(자기조직화지도)    인공신경망의 종류, 차원축소와 군집화를 동시에 시행\n  SOM 프로세스\n 연결 강도 초기화 입력 벡터와 경쟁층 노드 간의 유클리드 거리 계산 선택 노드와 이웃 노드 가중치 수정 2로 가서 반복, 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨    승자 독식 구조로 경쟁층에는 승자 뉴런만이 나타남\n  신경망은 에러 수정, SOM은 경쟁 학습\n  SOM은 비지도 학습\n  연관분석 지지도 : 교집합 신뢰도 : 조건부 확률 향상도 : 교집합/a확률, b확률\n 향상도가 1보다 크면 양의 관계, 1이면 연관성 없음  절차\n 최소 지지도 설정 최소 지지도를 넘는 두 가지 품목 찾음 최소 지지도를 넘는 세 가지 품목 찾음 반복 수행  Apriori 알고리즘\n  데이터 간의 연관관계\n  구매 패턴 등 분석\n  장점\n 결과 이해 쉬움, 비목적성 분석 기법 사용 편리, 계산 간편    단점\n 품목 수가 증가하면 계산량 증가 너무 세부화되면 의미없는 결과 도출 상대적 거래량이 적으면 제외되기 쉽다.    ","description":"","tags":null,"title":"ADSP 복습 2","uri":"/posts/certificate/2020-02-16-adsp_remind2/"},{"categories":["Certificate"],"content":"데이터의 이해 데이터와 정보 데이터 유형   정성적 : 언어, 문자\n  정량적 : 수치, 도형, 기호\n  암묵지 : 학습과 체험을 통해 개인에게 습득되지만 겉으로 드러나지 않는 상태의 지식\n  형식지 : 암묵지가 문서 등으로 표출되어 공유할 수 있는 지식\n  공통화 -\u003e 표출화 -\u003e 연결화 -\u003e 내면화 공통화 : 경험 공유를 통한 새로운 암묵지 창조 표출화 : 암묵지에서 구체적 개념 도출 연결화 : 형식지의 완성도를 높여 지식체계로 전환 내면화 : 형식지를 학습해 구체화된 개인 지식\n 위와 같은 과정을 거치며 지식의 발전을 기반으로 한 경영을 지식경영이라 함  데이터와 정보 관계 데이터 : 가공하기 전 순수한 수치나 기호 정보 : 상관관계 간 이해를 통해 패턴 인식, 의미부여(ex a마트 연필 가격이 더 싸다) 지식 : 정보 패턴을 이해하여 이를 토대로 예측한 결과물(ex 더 저렴한 a마트에서 연필을 삼) 지혜 : 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 것(ex 다른 상품들도 쌀 것이라 판단)\n데이터베이스 정의와 특성 데이터베이스 특징  통합된 데이터(중복x), 저장된 데이터, 공용 데이터, 변화되는 데이터  데이터베이스 특성  기계가독성, 검색가능성, 원격조작성 정보 이용, 관리 정보기술 발전, 경제 산업적 측면  데이터 베이스 활용 기업 내부 데이터베이스 1980년대  OLTP : 온라인 거래처리, 트랜잭션을 컴퓨터에서 처리하여 결과를 사용자에게 돌려줌 OLAP : 온라인 분석처리, 통계적 요약 정보를 제공하는 기술  2000년대  CRM : 고객으로부터 수익창출, 장기적 고객관계 SCM : 제조 등 유통공급망에 참여하는 모든 업체들이 협력을 바탕으로 정보기술 활용, 재고 최적화  분야별 기업 내부 데이터베이스  제조부문  DW : 데이터 웨어하우스, 정보검색 목적 ERP : 제조업 등 비즈니스 분야 BI : 기업의 DW에 접근해 경영활동에 활용 CRM   금융부문  EAI : CRM, ERP, SCM등이 상호 연동 가능하게 함 EDW : 기존 DW확장 블록체인 : 데이터 분산처리 기술 ERP, e-CRM   유통부문  KMS : 지식관리시스템 RFID : 무선 주파수 이용 CRM, SCM    DW 4대 특성 데이터 주제지향성, 데이터 통합, 데이터 시계열성, 데이터 비휘발성\n사회 기반 구조 데이터베이스  EDI : 전자문서를 만들어 교환 CALS : 광속 상거래 물류 : VAN, 의료 : EDI, 교통 : ITS, 교육 : NEIS, 지리 : GPS,NGIS  BI, BA\n   구분 BI(Business Intelligence) BA(Business Analytics     목적 과거의 성과를 측정,비즈니스 계획 데이터와 통계 기반 성과 이해, 통찰력 중심 분석   응용 데이터 기반 의사결정 사전에 예측, 최적화, BI보다 진보    데이터 가치와 미래 빅데이터 정의 크기, 다양성, 속도 1tb = 1024gb 1pb = 1024tb 1eb = 1024pb 1zb = 1024eb\n빅데이터 기능  석탄,철 : 차세대 산업혁명, 혁명적 변화 원유 : 정보를 제공해 생산성 향상 렌즈 : 데이터가 산업 전반에 영향 플랫폼 : 공동 활용의 목적으로 구축된 유무형 구조물  본질적 변화 사전 -\u003e 사후 표본 -\u003e 전수 질 -\u003e 양 인과 -\u003e 상관\n빅데이터 활용 테크닉  연관규칙학습 유형분석(분류) 유전 알고리즘 : 최적화의 메커니즘을 찾음 기계학습 : 학습 후 예측 회귀분석 : 영향 감정 분석 소셜 네트워크 분석(sna)  위기 요인, 통제 방안  사생활 침해 : 동의제 -\u003e 책임제 책임 원칙 훼손 : 책임 원칙 강화 데이터 오용 : 데이터 알고리즘 접근권 허용, 인증 방안  빅데이터 활용 3요소 데이터, 기술, 인력\n데이터 사이언스와 전략 인사이트 일차적 분석 애플리케이션 사례  금융 : 신용점수, 사기탐지, 고객 수익성 분석 소매업 : 재고 보충, 수요예측 제조 : 맞춤형 상품, 신상품 개발 에너지 : 트레이딩 공급, 수요예측 온라인 : 웹 매트릭스, 고객 추천, 사이트 설계  데이터사이언스 구성 요소  IT Analytics 비즈니스 분석  데이터 사이언티스트 요구 역량  hard skill  이론적 지식 분석 기술 숙련   soft skill  통찰력 있는 분석 : 창의적 사고, 호기심, 비판 설득력 있는 분석: 스토리텔링, 시각화 협력 : 커뮤티케이션    데이터 사이언스 : 과학과 인문의 교차로\n사회경제적 환경 변화  단순 세계화 -\u003e 복잡한 세계화 제품 생산 -\u003e 서비스 생산 -\u003e 시장 창조  인간을 바라보는 세 가지 관점  타고난 성향의 관점 행동적 관점 상황적 관점  가치 패러다임의 변화  디지털화 연결(사물인터넷) 에이전시 : 복잡한 연결을 얼마나 효과적으로 관리해주는가   데이터 분석 기획 분석 기획 방향성 도출 데이터 사이언스 역량 컴퓨터 사이언스, 비즈니스 분석능력, 수학통계학 지식\n분석 주제 유형  Optimization : 분석 대상, 방법을 이해 Solution- 분석 대상만 암 Insight : 분석 방법만 암 Discovery : 분석 대상, 방법 모름  분석 기획 방안    당면한 주제(과제 단위) 주제 지속적 분석 문화 내재화(마스터 플랜 단위)     Speed\u0026Test 1차 목표 Accuracy\u0026Deploy   Quick-Win 과제의 유형 Long term View   Problem Sovling 접근 방식 Problem Definition    Quick-Win : 즉각적 실행을 통한 성과 도출\n분석 기획 시 고려사항  가용한 데이터 적절한 유스케이스 탐색 장애 요소에 대한 사전 계획 수립 필요  분석 방법론 KDD 분석방법론  대상의 도메인에 대한 이해와 프로젝트 목표 설정 데이터셋 선택 데이터 전처리 데이터 변환 데이터 마이닝 데이터 마이닝 결과 평가  CRISP-DM 분석 방법론  업무 이해 : 목적 파악, 목표 설정, 계획 수립 데이터 이해 : 수집, 기술 분석, 탐색, 품질 확인 데이터 준비 : 선택, 정제, 통합, 포맷팅 모델링 : 알고리즘 선택, 파라미터 최적화 평가 전개 : 유지보수 계획, 보고서 작성  선택, 전처리 단계 -\u003e 데이터 이해 단계\n빅데이터 분석 방법론  분석 기획 데이터 준비 데이터 분석 시스템 구현 평가 및 전개  프로토 타입 : 제품의 원형, 검증을 거쳐야 시제품이 됨\n분석 기획 단계  비즈니스 이해 및 범위 설정  진행 방향 설정 후 프로젝트 범위 정의서인 SOW 작성   프로젝트 정의 및 수립  프로젝트 정의 : KPI(핵심성과지표), 목표 수준 구체화 프로젝트 수행 계획 : WBS 작성(일정별 계획)   프로젝트 위험 계획 수립  위험 대응계획 수립 : 회피, 전이, 완화, 수용    데이터 준비  필요 데이터 정의  데이터 정의 : 메타데이터 정의서, ERD   데이터 스토어 설계  정형 : RDBMS 사용, 데이터 매핑 비정형 : NoSQL, 하둡 사용   데이터 수집 및 정합성 점검  ETL, API, 크롤링으로 수집 API : 제공하는 기능을 제어할 수 있게 만든 인터페이스 ETL : 데이터 추출,변환, 적재의 약자, BI구현을 위한 구성요소    데이터 분석  분석용 데이터 준비 텍스트 분석 EDA MODELING : 분할, 모델링, 적용 방안 평가 및 검증  시스템 구현 - 설계, 구현, 시스템 테스트 및 운영\r 평가 및 전개 - 발전 계획 수립 및 평가 보고\r 분석 과제 발굴 하향식 접근 방법, 상향식 접근 방법 최적화 -\u003e 솔루션 발견 -\u003e 통찰\n하향식 접근 방법  문제 탐색 단계    비즈니스 모델 기반 탐색\n 업무, 제품, 고객, 규제와 감사, 지원 인프라    분석 기회 발굴의 범위 확장\n 거시적 관심의 요인  사회, 기술, 경제, 환경, 정치   경쟁자 확대 관점  경쟁사 영역, 대체재 영역, 신규 진입자 영역   시장의 니즈 탐색  고객 영역, 채널 영역, 영향자들 영역   역량의 재해석 관점  내부 역량, 네트워크 역량      외부 참조 모델 기반 탐색\n Quick\u0026Easy방식    분석 유즈 케이스\n 상세한 설명 및 해당 문제를 해결했을 때 발생하는 효과를 명시    문제 정의 해결 방안 탐색 타당성 검토  상향식 접근 방식  비지도학습 과정  분석 프로젝트 관리 방안 분석 과제 5가지 특성 관리 영역  data size data complexity speed analystic complexity accuracy \u0026 precision  agile : 과거의 방식(워터폴 모델)과 달리 일정한 주기를 가지고 끊임없이 프로토타입을 만들어내 필요할 때마다 요구사항을 더하고 수정\n분석 마스터 플랜 마스터플랜 수립 프레임워크 우선 순위 고려 요소  전략적 중요도 비즈니스 성과/ROI 실행 용이성  적용 범위/방식 고려 요소  업무 내재화 적용 수준 분석 데이터 적용 수준 기술 적용 수준  ISP : 정보전략계획, 정보를 포착해 전사적 관점의 정보 구조를 도출해 전략 및 실행 계획을 수립하는 전사적 종합정보 추진 계획\nROI 관점  VALUE : 비즈니스 효과, 나머지 투자비용 요소  과제 우선순위  시급성 : 3-4-2 난이도 : 3-1-2  나선형 모델 : 여러번의 개발 과정을 거쳐 점진적으로 프로젝트를 완성시키는 모델\n분석 거버넌스 체계 수립 분석 거버넌스 체계 구성요소  Process(과제 기획, 운영), System(IT 프로그램), Data(데이터 거버넌스), Human resource(분석 교육), Organization(조직)  빅브라더 : 정보의 독점으로 사회를 통제하는 권력, 체계\n분석 성숙도 모델(CMMI)  도입 : 환경과 시스템 구축 활용 : 결과를 실제 업무에 적용 확산 : 분석을 관리하고 공유 최적화 : 혁신 및 성과 향상에 기여  샌드박스 : 외부 접근을 차단해 제한된 영역 내에서 프로그램을 동작시킴(IT 최적화 단계) COE : 조직 내 새로운 역량을 만들어 확산하기 위한 전문가들 조합(조직 확산 단계)\n 높은 성숙도, 높은 준비도 -\u003e 확산형  데이터 거버넌스 체계 요소  데이터 표준화 : 표준용어 설명, 명명 규칙, 메타데이터 구축 관리 체계 : 메타 데이터, 데이터 사전 관리 원칙 저장서 관리 : 전사 차원 저장소 구성 표준화 활동 : 구축 후 주기적으로 점검  데이터 조직 및 인력 방안 수립  집중구조 : 별도의 분석 전담 조직에서 담당, 이중화/이원화 가능성 높음 기능 구조 : 별도 조직이 없고 해당 업무 부서에서 진행, 전사적 핵심 분석이 어려움 분산 구조 : 분석 조직 인력들을 현업부서로 배치, 신속한 ACTION가능, 역할 분담 명확히 해야함  ","description":"","tags":null,"title":"ADSP 복습 1","uri":"/posts/certificate/2020-02-14-adsp_remind1/"},{"categories":["Certificate"],"content":"다축 생성 다축 생성 절차  첫 번째 그래프 생성(축을 지정하지 않은 그래프) 점 추가 y축 생성 y축 이름 지정 두번째 그래프 생성 점 추가 y축 생성 y축 이름 지정 세 번째 그래프 생성 y축 생성 점 추가 y축 이름 지정 x축 생성 및 이름 지정 사용자화  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  plot(time,pop,axes=F,xlim=c(7000,3400),ylim=c(0,max(pop)), xlab=\"\",ylab=\"\",type=\"l\",col=\"black\",main=\"\") points(time,pop,pch=20,col=\"black\") axis(2, ylim=c(0,max(pop)), col= \"black\", lwd=2) mtext(2, text=\"Population\", line=2) par(new=T) plot(time,med,axes=F,xlim=c(7000,3400),ylim=c(0,max(med)), xlab=\"\",ylab=\"\",type=\"l\",lty=2,lwd=2, col=\"black\",main=\"\") points(time,med,pch=20,col=\"black\") axis(2, ylim=c(0,max(med)), col= \"black\", lwd=2,line=3.5) mtext(2, text=\"Median Group Size\", line=5.5) par(new=T) plot(time,grp,axes=F,xlim=c(7000,3400),ylim=c(0,max(grp)), xlab=\"\",ylab=\"\",type=\"l\",lty=3,lwd=2, col=\"black\",main=\"\") points(time,grp,pch=20,col=\"black\") axis(2, ylim=c(0,max(grp)), col= \"black\", lwd=2,line=7) mtext(2, text=\"Number of Groups\", line=9)   그 외 다양한 그래프  aplpack 패키지 : 줄기잎 그림, 체르노프 페이스, 스타차트 등 제공   줄기 잎 그림  1  stem.leaf(score)   얼굴 그림  1  faces(worldplace)   별 그림  1  stars(worldplace)    공간 분석 구글 비즈 모션 차트\n 구글 비즈를 사용하기 위해서는 그래프 축과 관련된 시간과 id변수를 지정한 후 plot으로 그림  1 2  m1 = gvisMotionChart(Fruits, idvar=\"Fruit\",timevar=\"Years\") plot(k)   지오차트\n 지도와 그 위에 데이터를 표시  1  gvisMotionChart(data, locationvar=\"\", colorvar=\"\",sizevar=\"\",howervar=\"\",options= list(), charid)    색상 구분  1 2 3 4  G1 = gvisGeoChart(exports, locationvar='Country', colorvar='Profit') # 전 세계 지도에 수익크기를 색상으로 구별 G1 = gvisGeoChart(exports,'Country','Profit',option=list(region=\"150\")) # 유럽 지역으로 한정해 구분   표시 방식 및 해상도 지정  1 2  G1 = gvisGeoChart(states,\"statename\",\"illiteracy\",option=list(region=\"US\"),displayMode=\"regions\",resolution=\"provinces\", width=600,height=400) # 주별 문맹률 정보가 나타나도록 option표시 방식과 해상도 수준 지정   속도 표시  1 2 3 4  G1 = gvisGeoChart(Andrew, \"Latlong\", colorvar='Speed_kt',option=list(region=\"US)) # 위치별 속도를 각각의 색상으로 표시함 G1 = gvisGeoChart(Andrew, \"Latlong\",sizevar= \"Speed_kt', colorvar='Pressure_mb',option=list(region=\"US)) # 색깔이 아닌 원의 크기로 표현   깊이 표시  1 2  G1 = gvisGeoChart(quake, \"Latlong\",\"depth\",\"mag\", option=list(displayMode = \"Markers\", region=\"009\",colorAxis=\"{color:['red','grey']}\",backgroundColor=\"lightblue\")) # 깊이와 진도정보로 표시   모자이크 플롯\n 복수의 categorical variable 분포 파악이 도움이 되는 시각화  1 2  mosaic(titanic,shade=T,legend=T) # 색상 추가   1 2 3  strucplot(titanic,pop=F) grid.edit(\"rect:Class=1st,SEx=Male,Age=Adult,survived=Yes\",gp = gpar(fill = \"red\")) # 특정 집단의 색상 지정    샤이니 사용  R에서 인터렉티브하게 웹 앱을 만들 수 있는 패키지 동적 시각화 자료를 웹으로 쉽게 배포 가능 독립형 앱을 호스트하거나 R Markdown문서에 포함하거나 대시보드 작성 css, html, javascript 작업 가능  샤이니 기본 구성  구조   header, body, footer 구조를 지닌 html과 유사 headerPanel : 제목과 주제 sidebarPanel : mainPanel에서 다룰 수 있는 컴포넌트가 들어감 mainPanel : 실질적으로 보여지는 부분  ui.R, server R code   샤이니를 실행하기 위해 ui.R, server R code 파일이 동일 디렉토리 안에 있어야 함 ui.R : 화면 구성, component class 설정 server R code : 코드들이 들어가는 곳, id값을 설정해 ui.R에 input, output값으로 작동  hello_shiny(ui.R)\n1 2 3 4 5 6 7  shinyUI(pageWithSidebar( headerPanel(\"hello shiny\"), sidebarPanel( sliderInput(\"obs\",\"Number of observation: \", min=1, max = 1000, value=500)) mainPanel(plotOutput(\"distPloy\")) )) # obs를 컴포넌트로 나타내 값은 1부터 1000까지, 기본값은 500   hello_shiny(server.R)\n1 2 3 4 5 6 7  shinyServer(function(input,output){ output$distPlot = renderPlot({ dist = rnorm(input$obs) hist(dist) }) }) # input, output만듬, distplot이라는 함수를 output으로 보내 renderplot 출력   샤이니 기본 사용법 시작과 종료   일반적인 방법으로 c:/test/shiny로 디렉터리를 만들고 각각의 ui.R, server R파일을 폴더에 넣어 관리함(동일폴더에 있어야 함)  1 2  setwd(\"c:/test/shiny/hello\") runApp() # 샤이니 실행    샤이니는 r server에서 브라우저로 별도로 운영되기 때문에 브라우저 종료 후 반드시 세션을 끝내줘야 함  input, output\n input, output으로 id를 만들고 안에 설정된 데이터나 그래프 등을 주고받는다.  1 2 3 4 5 6 7 8 9 10 11  shinyUI(pageWithSidebar( headerPanel(\"hello shiny\"), sidebarPanel( selectInput( \"Variable\",\"Variable: \", list(\"Cylinders\" = \"cyl\", \"Transmission\"=\"am\", \"Geers\"=\"gear\")) checkboxInput(\"outliers\",\"Show outliers\", FALSE)) mainPanel(h3(textOutput(\"caption\")), plotOutput(\"mpgPlot\")) )) # 세션 종료 후 재실행하면 server.r 코드가 mainpanel에 나타남   Slider\n 슬라이드 바는 inputID 지정 후 label, min, max, value, step, format을 지정 후 눈금 표시 여부를 ticks를 T/F값으로 지정하고 움직임에 따라 animate를 T/F로 지정  Tabsets\n 한 화면에 tab을 만들어 탭별로 다른 그래프나 테이블을 보여줄 수 있음  dataTable\n 자바스크립트 사용, 코드를 두 개로 나누지 않고 한번에 코딩 가능, 하지만 관리 등을 위해 나눠서 하는게 좋음  라이브러리 기반 시각화 구현(d3.js) d3.js특징  자바스크립트 기반 데이터 시각화 라이브러리 SVG객체, ,canvas객체 등을 기반으로 동작 css를 통해 객체의 레이아웃과 속성을 변경해 디자인적 요소를 조작할 수 있음 firefox, chrome, safari opera 에서 모두 테스트되어 한 동일한 코드에 일관적인 결과를 얻을 수 있다.  시각화 구현 절차  데이터 획득   로컬에 저장된 파일, DB, 웹에 공개된 데이터  데이터 파싱   csv, xml, json등의 형식을 파싱할 수 있는 API 제공  데이터 필터링   필요하지 않은 데이터 제거  데이터 표현   중요한 사항은 매핑의 scale scale이라는 객체로 데이터와 시각적 요소 간의 관계 정의 한 번의 구현으로 다양한 화면의 크기에서 동작해야하는 시각화에서 매우 유용 다양한 크기의 화면에 동일한 차트나 지도를 그릴 수 있다. 또한 동적으로 변경되어도 차트나 지도가 깨지지 않음  상호작용 추가   마우스 클릭, 키보드 입력 등을 인지, 처리할 수 있도록 함 상세하게 보여주기, 지역 포커싱 등 가능  시각화 구현을 위한 기본 개념  객체지향 언어라 객체를 생성해야함 d3.js는 데이터를 svg이미지로 시각화하는데 사용되는 함수를 모아놓은 파일과 같은 것   SVG   그림을 그리기 위한 HTML태그 각각의 모양을 일일이 함수로 정의 후 표현 해상도와 독립적, 이벤트 핸들링을 지원해 사용자와 상호작용이 필요한 시각화 구현  scale   시각화 그림들이 화면에 출력되는 과정에서 부자연스럽게 표현되는 것을 방지 크기와 컬러를 자동으로 조정해 ‘시각화의 최적화’를 도움 데이터 값을 건드리지 않고 데이터 값에 맞는 크기와 컬러 범위를 출력 장치에 맞도록 시각화 domain : scale 입력값 범위 지정 range : scale 출력값 범위 지정 .scale : domain, range로 설정된 scale을 통해 원하는 위치에 무언가를 놓는 것  막대차트로 시각화 구현  객체 생성 데이터 입력 스케일 정의 차트에 막대 추가   rect 객체 사용  레이블 추가   text 객체 사용  축 추가  파이 차트  d3.svg.arc : 파이 모양 구현  스캐터 플롯 히트맵으로 비교 시각화 구현  색상의 차이를 통해 데이터 표현 canvas 객체 필요 drawimage : canvas에 준비한 이미지를 출력  svg와 canvas 차이\n    svg canvas     용도 시각화를 구현하기 위해 사용    객체에 정보 저장 O X   다시 그리기 유리 불리   성능 낮다 높다     svg 객체는 화면에 출력한 모든 정보를 담고 있기 때문에 event handler를 연결할 수 있다. 사용자의 행위에 따라 필요한 객체만 화면에 그릴 때 유리하지만 모든 정보를 객체로 저장하고 있어 성능 문제가 발생할 수 있음  지도로 시각화 구현  좌표 정보를 데이터로 입력하고 path객체를 할당하는 방법으로 구현할 수 있다. path 객체를 통해 좌표 정보를 픽셀로 변환해 화면에 도형을 그릴 수 있다.  ","description":"","tags":null,"title":"ADP 공부하기 14","uri":"/posts/certificate/2020-02-13-adp_study14/"},{"categories":["Certificate"],"content":"시각화 구현 개요 빅데이터 시각화 구현  시각화함으로써 데이터의 분포와 성격에 대해 한 눈에 알기 쉬워 인사이트를 얻기 좋다  대표적 시각화 방법  시각화 플랫폼   전문 시각화, 시각적 분석 플랫폼은 주로 BI, 인텔리전스 분야에서 사용  기업별 대표 제품\n IBM : 코그노스 인사이트, 인포메이션 빌더스 MS : 파워피벗, 파워뷰 마이크로스트레터지 : 비주얼 인사이트 오라클 : 오라클 비즈니스 인텔리전스 엔터프라이즈 에디션 클릭테크 : 클릭뷰 SAP : 비주얼 인텔리전스 SAS : SAS 인터프라이즈 비즈니스 인텔리전스 그외 : 타블로, 팁코 스폿파이어 애널리틱스  기존 BI 플랫폼 : 주로 데이터 분석, 마이닝 기법을 통해 일정한 방식의 결과리포트를 생성하기 위해 시각화 기술을 활용 전문 시각화 플랫폼 : 지식 시각화 관점에서 데이터 시각화 기능 지원 적용 방법 : 플랫폼 설치, 구축 필요\n Gephi : 수많은 edge와 노드로 이루어져 복잡한 네트워크 형태를 시각화  시각화 라이브러리   라이브러리를 설치해 제공하는 API로 코드 작성 도구 리스트  인포그래픽스   웹서비스 형태로 제공, 제공되는 템플릿으로 구현 ICHART, Visual.ly, Visualize FREE 등   \n분석 도구를 이용한 시각화 구현 그래프 작성  ggplot을 통해 다양한 시각화  xy그래프  기본 XY그래프   전체적 내용 파악, 수많은 데이터가 있을 때 파악하기 어려움 데이터를 넣고 x,y축 지정 후 color구분하는 코드  1  ggplot(chickweight, aes(x=Time,y=weight, colour = Diet, group=Chick))+geom_line()   - aes : xy축 지정, 색과 그룹별 지정 가능\r- geom_line : 선그래프를 그리는 함수\r 포인트 그래프  1 2  h = ggplot(chickweight, aes(x=Time,y=weight, colour = Diet)) h + geom_point(alpha=.3)   - 위에서 투명도를 0.3으로 조정\r 스무스 그래프  1 2  h = ggplot(chickweight, aes(x=Time,y=weight, colour = Diet)) h + geom_smooth(alpha=.4, size=3)   개선된 그래프(포인트+스무스)  1  ggplot(chickweight, aes(x=Time,y=weight, colour = Diet)) + geom_point(alpha=.3) + geom_smooth(alpha=.4, size=3)   히스토그램  도수분포표를 그래프로 나타냄  1 2  ggplot(subset(chickweight, Time=21), aes(x=weight,fill= Diet)) + geom_histogram(colour=:black,bidwidth=50) + facet_grid(Diet ~.)    Time변수가 21인 것만 선택, weight 간격 50 facet_grid(Diet ~.)는 가로로 출력, facet_grid(.~ Diet)는 세로로 출력  포인트 그래프   데이터를 정적으로 보여주고 색상으로 특성을 파악할 수 있다.\n  기본 포인트 그래프\n  1 2 3 4  p = qplot(wt,mpg,colour=hp, data=mtcars) p+coord_cartesian(ylim=c(0,40)) p+scale_colour_continuous(breaks=c(100,300)) p+guides(colour = \"colourbar\")   -\u003e y축 범위 지정, hp범위 지정, hp수치에 따른 칼라바\n 치환 데이터를 이용한 포인트 그래프  일정 데이터만 그릴 때, 데이터가 많으면 복잡성이 올라가고 파악이 불가능한 경우가 발생 10건 만 추출, p에 m을 설정    1 2  m = mtcars[1:10,] p% + %m   막대 그래프  기본 막대 그래프 범주형 데이터를 factor로 변환  1 2  c = ggplot(mtcars,aes(factor(cyl))) c+geom_bar()    다양한 옵션 적용  1  c+ geom_bar(fill=\"white\",colour=\"red\")     막대 내부는 white 테두리 색은 red\n  히스토그램 형식에 적용\n  1 2 3  m=ggplot(movies, aes(x=rating)) m+geom_histogram() m+geom_histogram(aes(fill=..count..))   선그래프  시계열에서 많이 쓰임  1 2  b = ggplot(economics,aes(x=date,y=unemploy)) b+geom_line()    다양한 옵션 적용  1 2  b = ggplot(economics,aes(x=date,y=unemploy)) b+geom_line(colour=\"blue\",size=0.3, linetype=0.3)   linetype : 선의 종류\n효과주기  기본 효과 주기   히스토그램 : 그래프화 할 때 히스토그램으로 커트 등급 별로 나타냄  1 2  k = ggplot(diamonds, aes(caret,..density..)) + geom_histogram(binwidth=0.2) k+facet_grid(.~cut)   facet_grid(.~cut) : caret종류를 그래프 위쪽에 표시\n 막대 그래프  1 2 3  w = ggplot(diamonds, aes(clarity, fill=cut)) w= geom_bar() w=geom_bar(aes(order=desc(cut)))    선 그래프  1 2 3  f = ggplot(df,aes,(x=x,y=y)) f + geom_line(linetype=2) f + geom_line(linetype=\"dotdash\")   \n포인트 그래프  임의의 선 삽입(수평선)  1  p+geom_point(size=2.5) + geom_hline(yintercept=25,size=3.5)    포인트 모양 할당  1  p+geom_point(shape = 5)    포인트 모양 (문자) -\u003e k라는 문자 지정  1  p+geom_point(shape = 'k', size=3)    포인트 모양 없애기  p+geom_point(shape = NA)\r 25가지 SHAPE사용  1 2 3  df2 = data.frame(x= 1:5, y=1:25, z=1:25) s = ggplot(df2,aes(x,y)) s + geom_point(aes(shape=z),size=4) + scale_shape_identity()    선형 모델링  1 2 3 4  dmod = lm(price~cut, data=diamonds) cuts = data.frame(cut=unique(diamonds$cut),predict(dmod,data.frame(cut=unique(diamonds$cut)),se=TRUE)[c(\"fit\",\"se.fit\")]) se = ggplot(cuts, aes(x=cut,y=fit,ymin=fit-se.fit,ymax=fit+se.fit,colour=cut)) se + geom_pointrange()    박스로 강조  1 2  p = ggplot(mtcars,aes(wt,mpg)) + geom_point() p + annotate(\"rect\",xmin=2,xmax=3.5, ymax=25,fill = \"dark grey\", alpha=.5)    축 범위 지정  1 2  p = qplot(disp, wt, data=mtcars) + geom_smooth() p+scale_x_continuous(limit=c(325,500))    boxplot  1  qplot(cut,price,data=diamonds, geom=\"boxplot\")   -\u003e 가로로 눕히려면 아래 코드 추가\n last_plot() + coord_flip()\n  qplot  1  qplot(cut, data=diamonds, geom=\"bar\")   ","description":"","tags":null,"title":"ADP 공부하기 13","uri":"/posts/certificate/2020-02-12-adp_study13/"},{"categories":["Certificate"],"content":"시각화 방법 시각화 방법 개념 정보 구조화  정보 조직화   데이터를 수집, 정제하는 과정이 적절히 배분되지 않는다면 제대로 된 결과물을 도출하기 어려움 데이터 수집, 분류, 배열, 관계맺기의 과정을 거침 조직화해 배치할 때 정보의 가치와 유용성은 증가하고 전하는 이야기도 달라진다. 데이터 munging : 원 데이터의 구문을 분석, 정리하고 집단으로 묶거나 변환해 패턴을 식별  데이터 수집 및 탐색   원 데이터를 바탕으로 필요한 데이터를 추출하고 활용하는 절차를 거쳐야 함 데이터 editing : 유의미, 무의미한 데이터 선별   분류\n 구분 텍스트   줄 바꿈으로 행을 구분자로 열을 구분하는 텍스트 데이터임 csv는 쉼표, tsv면 탭으로 구분자는 공백, 쉼표 등 어떤 것이든 가능  json   자바스크립트로 표현된 데이터를 쉽게 전달할 수 있는 스트링으로 변형 가능함 자바스크립트에 의해 쉽게 인터프린팅됨, 복잡한 구조로 표현됨 배열과 복합 개체들을 나타낼 수 있다.  xml   확장마크업 언어, 수많은 종류의 데이터 기술 xml은 구조적 데이터를 설명함 html의 한계를 극복할 목적 스프레이트시트, 구성파일 등 네트워크 프로토콜에 일반적으로 포함되는 정보가 구조 데이터의 예시임    배열\n    래치방법 : 위치,알파벳, 시간, 카테고리, 위계 이상 5가지가 정보를 정리, 조직화\n 위치 : 정보를 공간적 위치에 배열   다양한 출처나 장소에 기반을 둔 정보를 조사하고 비교할 때 좋음 지도 뿐만 아니라 안내도, 학문 영역의 범위를 포함하는 것도 해당  알파벳 : 사전, 전화번호부 등을 정렬   1차로 분류하고 하위 분류에서 가나다 순으로 조직화 데이터 속성이 다양하면 알파벳 순이 보편적  시간 : 연도별 시간 순서   일정 기간을 조직화하는 최적의 원리 정보의 변화를 발견하고 비교할 때 좋음  카테고리 : 정보의 속성에 따라 분류   중요도나 주제가 서로 유사한 정보에 적합, 수치보다 색상 등으로 표현을 달리함 상품 분류, 서적 분류 등  가중치(위계) : 정보의 변화에 따라 데이터 값이나 중요도 순서로 조직화   단위나 수치 표현 가능    관계 맺기(재배열)   데이터에 의미를 부여하는 기본적인 과정, 시각화와 밀접함 분류된 데이터를 정보 수용자가 인식하기 쉽게 패턴을 만듬  정보 시각화    시간 시각화 분포 시각화 관계 시각화 비교 시각화 공간 시각화     막대그래프, 점그래프 파이차트, 트리맵, 누적 연속그래프 스캐터 플롯, 버블차트, 히스토그램 히트맵, mds, 스타차트,체르노프페이스,평행 좌표계 지도 매핑     시간 시각화    변화를 표현, 트랜드(경향성)으로 장기간에 걸쳐 트랜드 추적\n  분절형 : 데이터의 특정 시점, 시간 구간\n  연속형 : 기온 변환 같은 데이터\n 막대그래프   막대를 배치함으로써 상대적인 차이를 알 수 있음 색상 표시 : 값들의 차이가 미미하거나 막대의 수가 많은 때, 막대가 서로 다른 범위나 상태일 경우 색상 표시x : 모든 막대가 동일한 범위  누적 막대그래프   전체의 합이 의미가 있는 경우 사용 단어들의 바차트 : 단일 소프트웨어 봇이 모은 많은 단어와 이미지를 갖고 있는 편집 역사를 시각화한 것  점그래프   면적을 표시할 필요가 없어 더 적은 공간에 그려짐 점의 집중정도와 배치에 따라 흐름을 파악하기 용이함 두 변수의 연관관계를 볼 때 많이 씀  연속형 그래프   점그래프를 선으로 연결, 끊임없이 변화하는 현상의 추이를 볼 수 있음. 기울기가 급할 수록 변화가 큼 데이터의 포인트 수에 따라 점, 선을 결정해야함    분포 시각화    최대, 최소, 전체분포로 나뉨\n  최대와 최소는 순서 정렬에서 양 끝\n  분포정도라 전부 합치면 1또는 100%\n 원그래프   부분과 전체, 부분과 부분을 비율을 알아봄 조각의 합은 전체 인접하지 않은 조각을 비교하기 힘듬 텍스트와 퍼센티지를 포함시키는 게 좋음 면적으로 값을 보여주고, 수치를 각도로 표시  도넛차트   조각에 해당하는 수치는 면적이 아닌 길이로 표시  트리맵   영역 기반 시각화, 시각형이 수치임 사각형을 포함한 바깥의 영역은 대분류, 내부 사각형은 세부 분류를 의미 위계 구조가 있는 데이터나 트리 구조의 데이터를 표시할 때 활용 뉴스맵 : 구글 뉴스를 시각화, 내부 사각형을 선택하면 뉴스에 대한 내용을 이미지와 텍스트로 노출  누적 연속 그래프   시계열을 쌓아올린 형태 가로축은 시간, 세로축은 데이터 값 한 시점의 세로 단면을 가져오면 분포를 볼 수 있음 네임 보이저의 그래프를 통해 어느 시기에 얼마나 선택됐는지 알 수 있다.     관계 시각화\n  스캐터 플롯\n 시간적 변화를 볼 때 쓰지만 관계를 알아볼 때도 씀 포인트들의 관련성 여부를 시각적으로 알 수 있음 포인트가 많을 때 유용    버블 차트\n 세가지 요소의 상관관계를 표현 버블의 크기가 변수 면적이나 지름으로 판단 갭 마인더 : 유엔의 데이터를 바탕으로 인구 예측, 이동에 관한 정보를 공유    히스토그램\n 한쪽으로 치우친 것이 없다면 균일한 분포 가로와 세로축은 연속적      비교 시각화\n    시작점을 찾는 것이 중요, 모든 데이터를 한 번 훑고 흥미로운 것을 찾고 다른 점을 찾아가는 과정이 더 도움이 될 수 있음.\n  히트맵\n 여러가지 변수를 비교 한 칸의 색상으로 데이터를 표현 데이터가 많으면 적당한 색상과 약간의 정렬과정을 거쳐야 함 감정 히트맵 : 주식시장에 대한 전망을 sns의 사회적 주식 지수로 보여줌    체르노프 페이스\n 데이터를 사람 얼굴 이미지로 표현 보통 사람들에게는 혼란, 전문가의 흥미가 목적    스타차트\n 중앙에서 외부 링까지 이어지는 몇 개의 축을 그리고 전체 공간에서 하나의 변수마다 축위의 중앙으로부터의 거리를 수치로 나타냄 중심점은 축이 나타내는 값의 최솟값, 먼 끝 점은 최댓값    평행 좌표계\n 여러 축을 평행하게 배치 y축에서 윗부분은 값 범위 최댓값, 아래는 최솟값 측정대상은 위아래로 이어지는 연결선으로 그려짐    다차원척도법\n 개별 데이터 간의 유사도를 바탕으로 시각화 거리행렬을 포함하는 데이터의 시각화에 유용 유사성이 작으면 멀리, 크면 가까이      공간 시각화   지도를 만들어 위치 비교 여러 장의 지도를 통해 시간의 여러 단면을 표현할 수 있음 구글 차트의 지오 차트 : 매핑 포인트를 모르고 지명만 알아도 시각화 작업할 수 있음  그래프 그리기  그래프 유형 선택   범주와 측도(차원)의 수에 좌우됨, 최소한의 것으로 최대한 것을 전달해야 함  그래프 단순화  배경을 지우기 : 배경은 데이터를 강조하는 데 방해가 됨 범례 지우기 : 보는 사람이 직접 매칭해야 되서 해석에 방해 테두리 지우기 : 답답한 느낌, 디자인에 방해 색깔 지우기 : 다양한 색은 핵심이 무엇인지 파악하기 어렵게 함 특수효과 : 디자인 통일성파괴, 핵심 전달 방해 굵은 글씨 : 시선을 분산 라벨을 흐릿하게 처리 : 처음 보는 사람이 데이터의 핵심에 집중하게 해줌 보조선을 흐릿하게, 지우기 : 데이터를 표현하는 부분이랑 겹쳐 처리 라벨을 직접 표시 : 보조선을 없애고 라벨을 데이터에 직접 표시하면 즉각적인 해석 가능    정보 시각 표현  그래픽 요소 : 뇌는 먼저 비슷한 물체와 조금 달라보이는 것을 구분한 다음에 나머지를 자세히 탐색함  자크베르탱의 그래픽 7요소  위치   위치의 변화로 하나의 요소를 강조할 수 있음 수치로 표현할 수 있음, 정보의 상하구조를 효과적으로 전달  크기   하나만 작게 만들면 작은 요소에 집중 주위 크기와 다른지가 중요  모양   하나만 다른 모양으로, 전혀 다른 형태로 바꾸기 형태만으로 큰 대비 효과는 힘듬  색   하나만 다른 색으로, 대개 보색을 씀 수치 표현이 힘들어 순서를 매기기엔 적합하지 않음  명도   하나만 명도를 다르게 함, 색상보다 더 명시성에 영향을 줌  기울기   시선은 반복에서 벗어나 변화를 감지해 강조한다고 느낌  질감   지나치게 쓰면 좋지않음  시각화를 위한 그래픽 디자인 기본 원리  타이포그래피    1~2가지의 서체의 크기나 스타일의 변화를 줌\n  한글과 영문을 비슷한 느낌으로 주는 게 좋음\n서체\n 서체는 글의 형태를 총칭, 타이포그래피에서 가장 어려운 일 세리프는 가독성이 높아 본문용, 산세리프는 주목성이 높아 제목용  무게\n 두께를 의미,무게감으로 위계표현  크기\n 글자가 배치되는 금속 활자판의 높이 의미 서체에 따라 크기가 다를 수도 있음  스타일\n 가로 세로 비율, 각도에 따라 달라짐 글자 폭을 조정, 기울기  색채\n 정보의 중요도나 종속 관계 표현 가능 바탕색에 영향을 받고 청색은 후퇴돼 보여 자제  간격\n 가독성에 영향, 다음 글자가 다른 글자보다 밀접해야 함 글자 사이 \u003c 낱말 사이 \u003c 글줄 사이    색상    색채학 원칙을 알고 지킴, 균형을 깨서는 안됨\n  보색을 이용, 명도와 채도는 같게\n  컬러 스킴 : 적절한 보색 및 배색 색상 팔레트 추출\n  어도비 쿨러 : 공유된 단어와 관련된 배색 팔레트를 가져다 쓸 수 있음\n  스네이크 오일\n구분 표현\n 정보를 구분하고 묶음, 색은 사용 숫자를 제한할 필요가 있음 보통 사람이 구분할 수 있는 색상은 8가지  순서 표현\n 명암 단계, 스펙트럼 단계 등을 이용해 구분 섬세한 순서를 표현할 때 무채색의 단계가 정보를 더 명확히 전달 명도와 채도를 복합 개념이라 할 수 있는 톤은 선형적 단계를 표현 -\u003e 정보의 순서와 위계 표현  비율 표현\n 시각적 구별할 정도로 표현 가능 0을 중심으로 0을 중립적 명도로 표시하고 위, 아래 수치들은 상반되는 두 가지 색을 사용  색채 사용과 인지\n 색을 통해 이해할 때 인간의 지각와 인지 작용이 관여함 정보들이 충돌없이 인지될 때 정보의 해석이 빠름    그리드    디자인 안에 여러요소를 복합적으로 배치할 때 반드시 그리드를 계획하고 지켜야 함\n  모션 인포그래픽, 인터렉션 정보 디자인에서도 그리드는 중요한 요소\n  블록 레이아웃을 잡고 그 위에 요소를 효율적으로 놓아 전체적 조화 추구\n하나의 화면 읽기\n 눈이 움직이는 방향을 생각 습관적으로 상단 왼쪽의 입구를 보고 오른쪽 하단으로 내려감 색의 농도는 가장 강하게 주목하는 초점이 어디인지를 보여줌  정보의 역피라미드\n 가장 중요한 정보가 위로, 밑으로 갈 수록 중요도 낮아짐 보는 사람이 모든 텍스트를 읽지 않아 맨위에 중요한 것을 배치  망 그리드\n 수평선과 수직선의 연속이 개체를 배치하는 지침 일관성이 생기고 실험의 여지도 남겨 놓아 역설적으로 디자인이 쉬워짐  3등분 법칙\n 3*3 그리드를 포개 교차하는 곳을 적극적 핫스팟으로 삼아 역동적 결과를 배치 비례 간격을 끌어들여 미학적으로 만족스러운 균형이 잡힘 핵심 요소를 핫스팟 가까이에 배치하면 구성에 역동성을 더함    아이소타이프   많은 양의 데이터를 쉽게 지각함 국제적 그림언어 체계로 갖가지 지식을 조직적으로 시각화 정보, 자료, 의미를 나타내기 위해 상징적 도형이나 정해진 기호를 조합해 나타냄 단순한 픽토그램은 아님, 하나의 기호가 일정한 수량을 대표 인류의 전통에 기대고 있어 아이소타이프 도표의 기호들은 시공간을 초월해 읽혀야 함 이미지 활용, 최근들어 아이콘으로 발전  인터렉션  인터렉션은 사용자 스스로 정보를 탐색, 필터링하는 과정에서 인사이트를 얻을 기회 제공 인터렉션 위 정보 디자인은 비선형적 구조, 사용자가 임의로 정보에 접근해 선택적 탐색 가능 시간 제약이 없어 능동적, 전달 효과 높음 메시지도 다양하게 조직화 비선형적 구조에서 인터렉션 개념 적용이 중요 요점을 제시해 사용자들이 스스로 정보를 탐구해 이해하도록 유도 정보와 정보 사용자간 관계를 확장하고 심화  강조하고 디테일을 보여주는 방식\n 웹의 진화라는 시각화 프로젝트는 사용자의 적극적 개입 유도  사용자가 콘텐츠를 선택\n 사용자는 데이터 변환 컨트롤을 통해 다른 데이터세트를 불러오는 템플릿 위에 자신이 필요한 데이터만을 취사선택해 볼 수 있음 통계 그래프보다 더 많은 데이터 세트를 관찰할 수 있도록 함 변경의 경우 즉각적으로 반영됨 위계적 원 모양의 드릴다운 내비게이션 버블로 구성  여러가지 방법으로 데이터 보여주기\n 지도는 위치와 시간 흐름의 타임라인 강조(축소/확대 기능) 시각화는 버블차트의 콘텐츠 조정을 위해 선그래프 사용 사용자는 애니메이션으로 시각화를 실행하고 직접 인터렉션 해볼 수 있음  사용자 지정으로 시각 맵핑 변화\n 멀티 조정 시각화는 작은 공간이라는 제약을 벗어나 시간대 데이터의 다양한 관점을 보여줌 시각 데이터 재매핑을 지원하고 시각화 크기를 극대화 하도록 함 데이터가 맵 위의 시각 레이어를 프로젝션하는 방식  사용자 관점과 의견이 반영\n 사용자 주관적 관점과 데이터 표현을 혼합하는 시각화는 주제를 표현하는 사용자 반응 프로세스의 가장 중요한 부분 주제 측정이 사용자가 생각하는 주관적 지표에 의해 결정됨 개인적으로 중요하게 생각하는 것은 결과치를 소셜 미디어를 통해 공유할 수 있음  시각 정보 디자인 7원칙  시각적 비교를 강화 : 연관된 변수와 트랜드를 비교할 수 있는 도구 제공 인과관계 제시 : 디자인할 때 원인과 결과를 제시 다중변수 표시 : 연관된 변수를 활용해 정보 표현 텍스트, 그래픽, 데이터를 조화롭게 배치 : 라벨, 범례가 도표에 녹은 다이어그램이 효과적 질과 연관성, 진실성 : 정보가 과연 사용자의 특정 목적을 달성하는데 도움이 되는지 고민 시간순이 아닌 공간순 나열 : 사용자의 이해가 쉬움 정량성을 제거하지 않기 : 정량적 정보를 한 눈에 파악하게 하기   \n빅데이터 시각화 디자인  기업에서 빅데이터 시각화를 통해 제공하는 것  내부적 : 인사이트로 도출하는 시각적 분석 도구, 정보 전달 및 상황 진단 프로세스 개발 외부적 : 정보를 고객에게 전달   빅데이터 시각화는 데이터 또는 정보를 탐험할 수 있는 기회를 사용자에게 주는 형태여야 함 이미지 형태보다 인터렉션 형태의 결과물로 제공 ex) 페이스북 인기 지도  빅데이터 시각화 디자인 사례  2D 이미지   인쇄물이나 온라인 이미지 색상 및 그래픽 형태로 표현, 시각화 툴과 프로그래밍 기술 이용  모션 영상   시각화를 순차적으로 보이거나 자동 애니메이션 제공 모션 인포그래픽 ex)윈드맵, think 뉴욕 설치 광경  인터렉티브 애플리케이션   결과물은 인터렉티브 형태가 주류임 많은 양의 정보를 다양한 레이어나 필터를 통해 사용자가 접근하도록 함 ex) 갭 마인더, GBCS, 트룰리아, 에코매지네이션 챌린지  빅데이터 시각화 디자인의 방향  개인의 능력을 통해 최신의 기술과 도구를 사용해 정보를 제시하고 분석하는 것 비주얼 인식의 심리적인 부분을 알고 특정 시각화 기술이 줄 수 있는 한계에도 이해해야 함 궁극적으로 정보 디자인의 의도와 방향이 목적과 어긋나지 않아야 함 범주 안에서 정보를 보고 사용자를 위한 시각화 목적을 설정하고 이를 끝까지 고수해야 함 적합한 데이터 수집, 가공, 그래프 처리 과정이 진행되어야 하고 전문성이 단절되어선 안된다. 시각화에서 정보성이 결여되면 효율성과 참신성이 떨어지는 문제가 발생  ","description":"","tags":null,"title":"ADP 공부하기 12","uri":"/posts/certificate/2020-02-11-adp_study12/"},{"categories":["Certificate"],"content":"시각화 디자인 시각화의 정의 1.데이터 시각화의 중요성 시각화란?\n 매우 광범위하게 분산된 방대한 양의 자료를 분석하여 한 눈에 볼 수 있도록 도표나 차트 등으로 정리 통찰력을 얻기 위해서는 시각화한 서비스가 필요  2.데이터 시각화의 목적  데이터 분석, 의사소통  3.효과  정보를 습득하는 시간을 절감, 즉각적인 상황 판단 흥미를 유발하고 정보의 빠른 확산 기대 자료를 기억하기 쉬움  시각 이해와 시각화 1. 시각 이해의 위계 2. 데이터  사전적 정의 : datum의 복수형, 근거가 되는 사실이나 참고 자료를 의미, 정보와 혼용, 미디어에 저장된 형태 연구나 조사 등의 결과인 일종의 기초 자료, 정보를 만들기 위한 원자재 정보로서 가치가 부족해 분석이 대상은 되도 디자인의 대상은 될 수 없다. 불완전, 비연속적, 정보 전달 가치 없음  3. 정보  그 자체로 의미, 관점에 따라 다르게 전달될 수 있고 나름대로 형태와 형식을 가짐 패턴을 가시화, 의미를 전달 가치를 갖기 위해서는 변형과 조직화 필요 콘텍스트 : 데이터의 환경, 이해하는 사람의 환경과 태도가 무엇인지 설명 자기 조직화되지 않은 일반적인 의미만을 내재  4. 지식  다른 영역의 정보가 자기 조직화해 획득할 수 있는 것 경험을 통해 정보를 통합, 인위적으로 습득하는 고도의 논리적 상식 경험을 통해 형성된 지식은 세부 사항을 설명할 뿐만 아니라 다양한 상황에 적용할 수 있도록 일반화한 것  5. 지혜  고차원 방법으로 충분하고 이상적인 패턴을 이해하는 정보의 최종 단계 지식이 자기 내면화가 되서 개인적 맥락 안에 포함될 때 지혜가 됨 자기 내면화된 지식이라 상대방에게 전달하기 어려움 추상적이고 철학적, 인위적으로 전달하거나 공유할 수 없음  6. 정보 인터렉션 디자인  공급자는 데이터와 정보에 속하고 수용자는 정보와 지식 단계에 속함 정보 : 글로벌 콘텍스트, 지식 : 로컬 콘텍스트  시각화 분류와 구분 1. 데이터 시각화  그래픽 의미를 이용해 명확하고 효과적으로 의사소통하기 위함 복잡한 데이터보다는 직접적인 관점을 제공 연결과 그룹핑에 초점 두 가지 관점 : 통계적 그래픽,주제 지도학 마인드 맵, 데이터 표현, 관계 표현, 기사, 리소스, 툴과 서비스  2. 정보 시각화  시각적 표현 방법의 간학문적 연구 영역 대규모 비수량 정보를 시각적으로 표현 시각적 표현 방법과 인터렉션 기술을 통해 정보를 직관적인 방법으로 전달하기 위해 접근 방법을 창조 표현 : 분기도, 수지도, 히트맵 , 트리맵  3. 정보 디자인  사람이 사용할 수 있는 효과적인 정보를 시각적으로 표현 데이터, 정보 시각화도 정보 디자인에 속한다고 할 수 있다. 인포그래픽, 다이어그램  4. 인포그래픽  중요한 정보를 한 장의 그래픽으로 표현해 사람들이 손쉽게 정보를 이해할 수 있도록 하는 그래픽 메세지 패턴이나 트렌드를 봄, 기초, 지도, 기술문서 등에서 이용 뉴스 그래픽이라고도 불리고, 원 데이터를 취급하지 않음 차트, 다이어그램, 일러스트레이션 등 사용 정보형 메세지  객관적인 정보 전달 지하철 노선도와 같이 실제 지도를 왜곡하더라도 보기 쉽도록 구현   설득형 메세지  주장하는 바를 알리는데 목적 정보 전달보다는 시각적으로 강렬하게 주장하는 바를 전달 사회 계층별 분포 데이터를 옷에다 극단적으로 함축한 예시가 있음    빅데이터 시각화 영역  메시지 전달 관점에서 시각화  데이터 시각화 : 많은 데이터에 의미부여해 효율적으로 전달 정보 시각화 : 큰 범주에 해당하는 정보를 시각화   정보 디자인은 데이터의 디테일보다는 그래픽을 적극적으로 이용해 시각 스토리텔링 형식의 설득형 메시지를 전달하는 것에 초점 객관적 표현에 초점 직접적으로 전달하는 기능성에 초점을 맞춘 정보형 메시지를 전달하기 위한 데이터 시각화를 하는 경향이 강함. 설득형 메시지인 경우 인포그래픽에 해당하는 결과물이 도출될 수 있음  정보 디자인에서 빅데이터 시각화 영역\n 양적 정보 디자인은 정보 시각화와 겹치면서 데이터를 객관적으로 비교해 인과관계를 왜곡없이 전달하는데 초점을 둠 정보의 내용과 환경이 복잡하므로 표현도 다차원적이어야 함 어떤 식으로 해석하는 가에 대한 통계적 차원의 시각화 방법 및 이에 따른 시각 표현이 병행되어야 함.   시각화 프로세스 정보 디자인 프로세스 인포그래픽 디자인 10단계  프로젝트 수주\u0026니즈 파악 메타데이터 읽기 보충자료 요청 자료분석\u0026이야기 찾기 초안 작성 글쓰기\u0026이미지 취합 디자인 시안 작성 검토\u0026수정 납품  정보 디자인 프로세스 10단계   데이터 자체를 수집하는 단계\n 스토리를 시작하는 단서를 찾음 큰 그림은 떨어진 차트 안에서가 아닌 흩어져 있는 다양한 리소스들로부터 발견됨 시각화 전문가가 원 데이터를 직접 수집하기는 어려움    모든 것을 읽기\n 에코시스템 안에서 정보의 작은 조각들을 큰 그림으로 맞추는 것이 중요 정보가 빠졌는지, 근거를 확인하는 등의 노력 필요    내러티브 찾기\n 내러티브 : 스토리텔링, 실화나 사건을 묘사, 이야기를 조직하고 전개하기 위해 이용되는 각종 전략, 형식을 포괄 인포그래픽은 복잡한 데이터를 명료화하고 프로세스를 설명하고 트렌드를 창조, 논란의 부분을 보조하는 의도와 함께 시작된다.    네레티브를 찾기 위한 질문\n 제공하고자 하는 스토리를 만들어 내는 것이 가능한가? 이 주제에 관심이 가는가? 주목할 만한 사실, 가치를 말하고 있는가?   문제의 정의\n 결과에 대한 논리성을 검토 컬러, 타이포그래피 등의 주관적인 관점에서 디테일을 만들어감 수용자에게 정보의 진실을 알아가는 경험을 하게 해야함    계층 구조 만들기\n 이야기의 중심을 찾으면 프로젝트를 정리하고 개별 자료를 정리 중요한 것은 주인공으로, 나머지는 보조적인 요소로 배열, 해당 요소들이 리서치 단계에서 콘셉트 보드 역할을 함 이 단계에서 최종 결과물이 나타나기 시작    와이어프레임 그리기\n 계층 구조가 결정되면 와이어프레임이 창조됨 시각표현을 만들어내 정보의 계층 구조를 이해하게 만듬    포맷 선택\n 가장 좋은 접근 방법은 전통적인 파트와 그래프를 이용하는 것 프로세스를 설명하기 위해 다이어그램, 흐름도 등이 필요 간단한 숫자를 나열하는 것도 충분 충분한 예산이 있다면 인터렉션을 시도 결정은 이러한 포맷들의 복합이거나 한 가지 종류의 포맷을 불문하고 그것이 갖고 있는 데이터에 의해 선택되야됨    시각 접근 방법 결정하기\n 차트와 그래프를 일러스트와 함께 보여주거나 전통적인 데이터 표현과 시각 표현을 함께 사용하기도 함 정보, 매체, 클라이언트, 브랜드, 주제는 궁극적인 해결 방법을 결정하는 요소들 직관적이고 효과적인 인포그래픽을 결정하기 위해서는 두 가지 시각적 접근 방법 필요 초기 데이터의 아름다움을 만들어내는 방법 : 컬라나 타이포그래피, 각 조각들을 연결한 구조들은 추상화 느낌 일러스트레이션이나 메타포를 이용한 방법 : 사진 등 다양한 자료로 시각화, 어떤 시각적 접근 방법이 목적에 맞는지 결정    정제와 테스트\n 최종 결과물 디자인이 원래 의도와 목적에 맞게 데이터와 시각적 스토리텔링이 잘 되었는지 확인 쉽게 이해되는지의 여부 확인 평가하고 가능하면 디자인을 간단하게 개발하고, 반복 테스트 과정이 진행되어야 함    세상에 선보이기\n 온라인을 통해 대중들에게 배포 모든 사실 조사와 상상이 스토리의 모든 관점을 드러내는 것은 아님. 새로운 방식으로 해석될 수 있어 프로젝트가 결코 끝난 것으로 보는 게 아님 새로운 데이터가 나타나면 프로젝트는 프로세스 안에서 지속됨    빅데이터 시각화 프로세스 시각화 방법론  정보 디자인 교과서 : 4단계 방법론  조직화된 데이터, 시각적 매핑, 시각적 형태, 전달 방식   마티아스 샤피로 : 3단계  질문 만들어내기, 데이터 수집, 시각적 표현 분석보다 적당한 주제 선별 어느 정도 정해진 범주를 두고 시작하는 것이 아니라 시각화를 다루는 사람에 의해 좌우됨 질문과 수지의 단계가 바뀔 수도 있다.   벤프라이 : 7단계  프로세싱을 통해 프로그래밍으로 시각화 데이터를 모으고 분석하는 단계가 세분화       단계 단계 설명 관련 분야     1. 획득 파일, 디스크 등에서 정보 수집 컴퓨터공학   2. 분해 의미를 바탕으로 구조적으로 카테고리화 컴퓨터공학   3. 선별 위 과정을 바탕으로 필요 없는 정보 제거 수학,통계학, 컴공   4. 마이닝 데이터를 분석해 추출 알고리즘 도출 수학,통계학, 컴공   5. 표현 정보를 효율적으로 표현할 수 있는 방법 연구 그래픽 디자인   6. 정제 규칙을 바탕으로 정보를 시각적으로 정제 그래픽 디자인   7. 상호작용 다양한 시각에서 시뮬레이션 할 수 있는 방법을 반영 정보 시각화,HCI    방법론 연계    정보 구조화\n 수집 및 탐색, 분류, 배열, 재배열의 4단계로 나뉨 구조화 방법이 미리 학습되어야 함 기계학습 알고리즘으로 파악 후 자동 시각 추출하는 방법도 연구 중임 통계적 접근으로 시각화하는 것과 시각화 목적으로 정보 구조화에 접근하는 것은 다름.    정보 시각화\n sas, 하둡, 타블로 등 툴 사용 그래프를 왜, 어떻게 표현하는지 설명    정보 시각 표현\n 시각 표현을 극대화하고 다듬는 방안을 실험 데이터 시각화를 탐험할 수 있게 하는 다양한 벙법을 습득할 필요가 있음    빅데이터 시각화 프로세스    단계 단계 설명     1. 정보 구조화 수집하고 정제 과정, 분석 도구 필요   2. 정보 시각화 의미를 바탕으로 구조적으로 카테고리화   3. 정보 시각 표현 분석 도구에서 결과물에 별도 그래픽 요소를 추가해 완성    빅데이터 시각화 도구  엑셀 구글 차트  속도 빠르고 모바일에서 잘 작동 구글 스프레드 시트 저장, 초보자 위한툴 제공 차트 api가 있음   infogr.am  30종 차트, 무료 사용 pnf, pdf 추출 가능, 온라인 공유 용이   매니아이즈  11개 등을 제한된 범위 내 사용 데이터 셋 올리고 차트 선택   D3.js  시각화 프레임워크, html, css 사용   파이썬  대규모 데이터, 큰 계산 다룸 작은 코드로 많은 기능 가능, 다양한 라이브러리 탐색 단계라면 훌륭한 툴임   processing  디자이너와 데이터 아티스트의 활용 목적인 언어 몇줄의 코드로 애니메이션과 인터렉티브 그래픽을 만듬 자바로 만들어져 느리지만 최근 자바스크립트로 만들어주는 프로세싱 버전이 공개됨   R  데이터 통계분석 위주, 패키지가 많음 대다수 통계학자와 분석가들이 선호하는 프로그램 인터랙티브 그래픽과 애니메이션이 취약, 한 번 더 수정해야하는 단점이 있음   YFD  온라인 애플리케이션 시각화 툴 트위터 데이터 수집하여 시각화 도구로 패턴 및 관계를 찾는다. 개인적인 데이터를 다루기 위해 만들어짐   일러스트 레이터  벡터 기반 그래픽을 제작할 때 편리 PDF포맷으로 추출된 것을 편집하는데 이용       수준, 종류 구분 시각화 도구     기초 엑셀, 구글 차트, d3, visual.ly, cvs/json, Raphael   인터랙티브 GUI컨트롤 crossfilter, Tangle   매핑 Modest Maps, Openlayer, kartograph, polymaps   전문가 Processing, R, Weka, Gephi, NodoBo    ","description":"","tags":null,"title":"ADP 공부하기 11","uri":"/posts/certificate/2020-02-08-adp_study11/"},{"categories":["Certificate"],"content":"활용  내부에서 적용   활용되는 과정에서 새로운 통찰을 찾을 수도 있고 기존 통찰에서 부족한 점을 보완할 수 있다. 새로운 문제 해결 방식 도입과 구체적인 탐색과 발전의 과정 새로운 변인을 추가하거나 관련된 상수값을 보정, 서비스 개선 요소의 모델을 발견해 실행에 옮길 수도 있음 통찰은 보통 형태가 없어 시각화하는 것이 중요하다. 실행에 옮길 때는 현실적인 여건을 충분히 반영했는지 검토함  외부에 대한 설명,설득화 시각화 도구   쉽게 받아들이기 위해 일단 쉽고 간결해야함 통찰을 보다 효과적으로 전달하기 위해 시각화한 그림이나 그래프 활용 상대방이 공감해야 하므로 좀 더 강력한 상호 작용 필요 -\u003e 디자인 중요 직접 조작하며 메세지를 받아들이는 인터렉티브 인포그래픽과 정보 디자인을 시각화 도구로 이용하면 효과를 높일 수 있다. 탐색의 내려다보기와 올려다보기와 같은 조작을 통해 인터렉티브 인포그래픽은 메시지 생산자가 정해 놓은 범위 안에 수용자가 정보를 구체적으로 이해할 수 있게 함  인사이트의 발전과 확장 1. 탑다운vs보텀업\n 처음으로 무언가 살펴볼 때는 보텀업 의미있는 것을 파악해 추가로 얻어낸 정보를 토대로 탑다운으로 검증  2. 2차 잘라보기, 달리보기, 내려다보기, 올려다보기\n 기존에 도출한 데이터의 현실성 및 분석에서 활용한 모델의 적정성 체크 실세계에서 활용한 뒤 추가적으로 정보를 얻을 수 있다. 과정을 통해 시도해보지 않은 차원들 간 조합이나 특정 차원을 특정 값으로 고정해 보면서 인사이트를 고도화하고 확장할 수 있다.  3.실시간 vs 비실시간\n 일정한 시간이 지난 뒤 그것이 어떠한 영향을 가져왔는지 검증하는 과정의 반복이 필요함 반복 주기에 따라 실시간으로 탐색하고 분석할 수 있는 환경을 구축하는 것이 나은지 비실시간이라도 주기적으로 새로운 데이터를 구축해 환경을 구축하는 것이 나은지 결정해야 함. 데이터가 클수록 실시간으로 보는 것이 어렵다. 변화와 경향을 주기적으로 보는 것이면 굳이 실시간 환경을 구축할 필요는 없다.  4. 지표의 운영\n 지표를 활용한다는 것은 여러 가지 관계를 다 살펴보는 부담을 덜어준다 몇 가지 지표만 집중해도 전체적인 흐름을 알 수 있다. 인사이트 프로세스에서 추출한 지표를 중심으로 운영할 경우 문제점이 발생할 수도 있다.(환산된 값 중심이라 어떤 변화요인이 지표 흐름에 영향을 미쳤는지 찾기 어려움) 지표의 장점과 단점에 대해 명확히 이해하고 인사이트의 발전과 확장 효율성을 높일 수 있는 방향으로 지표를 끌고 갈 필요가 있다.  5. 추가 데이터에 대한 필요성\n 기존에는 살펴볼 생각도 하지 못한 관계들이 어떠한 데이터를 통해 파악될 수 있을 것만 같을 때 추가 데이터가 필요함 기존의 인사이트를 발전, 확장시키는 새로운 인사이트 프로세스에 추가 데이터를 반영할 계획이라면 일단 그 데이터가 정말로 필요한지 어떻게 사용할 것인지를 명확하게 짚어보아야 한다.  6. 시각화 문제\n 시각화 도구의 장단점과 적용 구조를 제대로 이해하지 못하면 치명적인 독이 됨 눈의 착각 : 사람의 시각이 물리적인 자극에 대해 있는 경우 그대로 정확히 받이들이지 못함 특히 인포그래픽 등으로 설명, 설득하는 경우에는 여러 가지 디자인 요소가 결합되어 오류가 발생할 여지가 크다 여러 관점에서 신중하게 고려해야 함  7. 사람의 문제\n 세부적인 방힉과 도출된 결과물은 질과 방향 등에 있어서 개인차가 발생할 수 밖에 없다. 특히 눈여겨 봐야 할 것은 전통적인 접근 방식과는 다른 생각이다. 새로운 가치와 통찰은 서로 다른 것들을 연결하는 것에서 태어난다는 것을 주목해야 한다.  ","description":"","tags":null,"title":"ADP 공부하기 10","uri":"/posts/certificate/2020-02-01-adp_study10/"},{"categories":["Certificate"],"content":"시각화 인사이트 프로세스 시각화 인사이트 프로세스 의미 사전적 의미로 정보, 인과관계, 본질 , 이해\nDIKW피라미드 데이터 : 개별적 기초 자료(원자료) EX 강수량 정보 : 데이터 간의 관계(상관,인과 관계) EX 지역별 연간 강수량 지식 : 다양한 정보가 상위 관계를 맺고 조직화 EX A마을의 수해대책 지혜 : 개인화된 지식,경험 등과 관계를 맺을 때 구조화되어 나타남 EX A마을 주민 개개인의 생활 노하우\n시각화와 인사이트 관찰 : 대상들 사이의 상호작용을 바탕으로 의미있는 관계를 찾아냄 성찰 : 자신의 내면 세계를 살펴봄, 자신의 사고와 행동에 의문을 제기하고 해결 통찰 : 관찰과 성찰을 기반으로 요인들 간의 관계를 통해 살펴봄 위의 삼찰을 바탕으로 대상들 사이의 숨겨진 관계를 찾아내는 과정을 통해 인사이트 얻음\n통찰 과정과 시각화   통찰과 시각화\n 통찰은 살펴보고 이해하는 과정 인사이트는 활용 과정에서 검증이나 보완할 수 있다. 통찰 과정의 시각화 : 눈에 확 띄게 만듬, 추상적 개념을 보이게 함 시각화 인사이트 프로세스 : 시각화를 통해 통찰을 추출하는 과정    1단계 탐색 - 관계 발견\n 어떤 관계가 있는지 최초로 살펴보는 단계 지혜를 통해 도출, 데이터에서 정보를 도출, 정보에서 지식을 도출 시각화로 객관적인 패턴을 발견하고 개괄적 패턴 찾기 검증 : 결과가 얼마나 효율적으로 도출되었는가    2단계 분석 - 관계 규명\n 관계들의 형태를 명확하게 규명하고 형태가 지니는 의미를 찾아냄 구체적 관계를 찾거나 관계를 보다 잘 설명하는 다른 요인을 찾는 작업 필요 방향성, 명제, 모델링, 지표 개요가 명확해야 함 정성적 기법, 정량적 기법 사용 시각화로 관계의 구체적인 모델링 및 적용, 조정 검증 : 분석의 결과의 효율성    3단계 - 활용 - 통찰 검증 및 보완\n 실제로 활용함으로써 얼마나 의미가 있고 가치를 인정받을 수 있는지 검증 부적절한 부분은 다시 탐색과 분석을 함 내부 : 직접 활용 외부 : 타인에게 설명 시각화로 타인에게 효과적으로 설명, 메시지 전달 검증 : 수용자가 제대로 이해했는지, 예상한 반응을 보이는지    탐색 사용 가능한 데이터 확인   데이터 명세화 : 차원과 측정값\n 모든 데이터는 기본적으로 하나 이상의 측정값과 차원을 가짐 EX\u003e 국가별 남성 평균 수명 -\u003e 차원 : 국가,성별/측정값 : 평균수명 연속적인 데이터로 구성된 차원은 구간 형태로 제시되기도 함 동일한 데이터 항목이라도 차원이 될 수도 있고 측정값이 될 수도 있다.    데이터 구성 원리1 : 이벤트 기록으로서 접근\n 원본 데이터는 특정 이벤트가 발생했을 때 발생한다. 로그 데이터와 로그 데이터를 한 번 정제한 데이터는 구분할 수 있어야 함 데이터가 어떤 원리로 생성,구성되었는지를 항상 염두에 두어야 함 관계는 시각화 도구로 찾아낼 수 있다.    데이터 구성 원리2 : 객체지향 관점에서의 접근\n 데이터의 구성과 생성 배경에 대해 고민함 데이터의 대략적 범위가 주어지면 데이터의 구조 자체를 설계,생성 하여 이를 토대로 통찰을 뽑아낼 수 있어야함 기본적으로 대상을 객체화 하고 모든 객체들은 행위와 고유속성값을 가짐 구조와 행위를 통해 구조 전체를 파악하는 것이 객체지향 관점 구조 전체를 파악해 그 구조가 제대로 이벤트 로그 데이터로 기록되고 있는지를 검증해 보완할 수 있다. 다양한 통찰을 위해선 데이터의 구성을 밝히고 추가 자료, 인사이트 프로세스의 목표 및 방향성을 조정하는 것이 필요    연결 고리의 확인 2개 이상 데이터를 활용할 수 있을 때는 연결고리를 살펴 관계의 범위와 방향을 정하고 확장할 수 있다. 이 때 연결 고리는 시각화 도구가 아는 데이터의 태성을 정리한 명세서에서 확인\n 공통 요소 찾기   서로 다른 데이터 명세서에서 공통 항목을 찾음 항목명이 아닌 항목의 정의와 데이터형을 보고 찾아야 함. 항목명이 달라도 같은 데이터형으로 되어 있고 기록된 규칙이 같다면 공통 요소이다.  공통 요소로 변환하기   데이터형이 다른데 공통 요소로 만들 수 있음 계층이나 기준으로 묶인 데이터의 대부분은 형태를 변환해 연결 고리를 찾음 자세한 자료를 덜 자세하게 묶인 자료 변환은 가능하지만 반대는 불가능 만드는 과정도 인사이트 프로세스 현실세계의 거의 모든 데이터는 구성 원리에 의해 시간과 공간 관점의 연결고리를 기본적으로 가짐  시간 데이터 변환\n 초 단위 데이터는 손쉽게 시간 단위, 날짜 단위, 분기 및 연 단위 등으로 전환 가능 날짜 시간 데이터가 문자열로 지정된 경우도 있음 -\u003e 시간 형으로 변환 DATE, YEAR, MONTH 등의 함수 이용  공간 데이터 변환\n 주소/주소를 세부적으로 구분한 행정구역(시,도), 가장 구체적인 좌표값 데이터에 따라 경위도 좌표계가 아닌 다른 기준의 좌표계로 구성된 경우도 있다. 텍스트 나누기, 문자열 함수 등 사용     함수명 함수 사용 형태 함수 기능 설명     SPLIT split(문자열, 구분자) 문자열을 구분 문자 기준으로 분리해서 제공   FIND find(찾는 문자, 문자열) 찾는 문자가 왼쪽에서부터 몇 번째에 위차하는지 숫자값   LEFT left(문자열,개수) 왼쪽부터 정해진 개수만큼 제공   MID mid(문자열,시작 위치,개수) 시작 위치부터 정해진 개수만큼 제공     지오코딩 : 좌표계를 주소 및 행정구역으로 변환하거나 반대 과정 코로플레스 지도 : 미국이나 유럽을 분석하기에 유용한 시각화 도구 X-Ray Map : 비즈 GIS가 무료로 제공하는 웹 GIS 도구, 한국 지역 유용  일정한 규칙을 가진 분류형 데이터로 변환\n 어떤 데이터는 하위 수준에서 기록되어 있고 다른 데이터는 상위 수준으라면 상위 수준이라는 공통 요소로 반환해 연결고리를 만들 수 있음 replace : 전체를 일괄적으로 바꿈 lookup, vlookup : 전체를 일괄적으로 바꾸지 않고 원하는 영역만 바꿈  탐색 범위의 설정   보유한 데이터를 조합을 고민, 명세화 해야함 여러 개의 데이터 명세를 보유한 경우 연결 고리를 확인해 탐색할 수 있는 차원과 측정값의 조합을 정리해야 함 각 조합 하나하나가 통찰을 추출하는 관점이 됨, 전체 조합 종류가 탐색의 범위  탐색 범위 설정 시 고려 사항\n 여러 개의 데이터를 보유한 경우 개별 데이터 안에서 먼저 탐색 측정값에 하나의 차원만 연결해 탐색 같은 데이터 안에서 차원과 측정값을 맞바꾸면 다른 통찰을 찾아낼 가능성 있음 목표와 관련있을 법한 조합을 만듬 상식적으로 의미나 연계성 없는 조합은 배제  관계의 탐색 1. 이상값 처리\n 측정 오류로 오차가 들어간 경우 제거 대상이 됨. 하지만 의미있는 이유일 수도 있어서 우선적으로 시각화 도구로 전체 구조를 파악하고 패턴을 찾아봄 기록 관리 과정에서 문제 -\u003e 보완, 대체, 제거 의미있는 이유 -\u003e 구체적으로 파고들어야할 대상  2. 차원과 측정값 유형에 따른 관계 파악 시각화\n 시각화 도구 선정  차원과 측정값이 어떤 유형인지 봄 1차원 선형, 2차원 평면, 3차원 공간에서 표현 시각화 도구 선정 시 고려 사항     차원은 반드시 축으로만 표현되는 것은 아님 2차원 평면에서는 x,y축 이외에 도형의 면적도 연속값으로 된 차원을 처리할 수 있는 도구로 사용 3차원은 입체의 부피나 단멱의 면적을 연속값으로 처리 색상 : 차원을 구분, RGB값으로 나눠 차원을 그라데이션 변화로 표현 가능  시각 데이터 관계 탐색   변화하는 패턴을 분리하는 것이 핵심 모션 차트 : 구글 스프레드 시트에서 제공, 움직임을 통해 보여주는 동적인 시각화 도구  공간 데이터 관계 탐색   실제 지도를 활용하는 것이 가장 직관적이고 효과적 Arc GIS : 유료화된 전문 지리정보 분석 도구 X-Ray Map : 무료 도구, 실제 지역 데이터 관계 볼 수 있음 파워 맵 : 엑셀 2013 도구, 모션 차트까지 결합해 제공  비정형 데이터 관계 탐색   우선 텍스트 문장들 안에 어떤 의미를 지니는 단어들이 어떤 빈도로 분포하는지를 살펴야한다. 워들 : 텍스트 데이터에서 형태소 단위를 추출해 빈도에 따라 색상, 크기를 결정하고 시각적으로 겹치지 않게 적절히 배치  3. 잘라보고 달리보기\n slice : 패턴을 탐색 후 일정 기준으로 일부분만 보는 것 dice : 차원들을 기준으로 잘라내 서로 다른 관점의 단면들을 살펴보는 것 피벗, 피벗 테이블 파워뷰 : 엑셀 2013기능 시각화 탐색을 적용하는 것에 비해 훨씬 강화되고 확장된 기능 OLAP : 기업에서 쓰는 BI도구, 실시간 기업 다차원 데이터에 접근에 slice, dice하며 분석, 리포팅 하는 도구  4. 내려다보기 올려다보기\n drill down : 하위계층으로 기준을 세분화 reverse drill down : 상위 계층의 관점으로 보는 것 상위하위 계층의 패턴을 살피고 그 차이점을 토대로 다시 하위 계층을 살펴보는 구조 트리맵 : 면적을 이용해 차원을 표현한 도구, 하이퍼볼릭 트리  5. 척도의 조정\n 정량적 데이터를 뿌려 놓을 때도 척도를 어떻게 설정하느냐에 따른 다름 측정값 범위가 너무 달라 패턴이 제대로 나타나지 않는 경우가 종종 있다. 실제 값을 변형해 같은 공간에 표시해도 각각의 패턴이 명확하게 보이게끔 조정해야 함 스파크라인 차트 : 계열별로 다른 범위의 측정값들을 동일한 공간 범위 내에서 패턴변화를 비교해 볼 수 있도록 자동으로 조정해 주는 시각화 도구  분석 분석 대상의 구체화  1차 탐색   어떤 패턴이 좀 더 중요하고 더 제대로 뜯어봐야 하는지 우선순위를 결정해야 함 찾아낸 단서들을 기반으로 우선순위를 조정해 볼 수도 있다. 궁극적 목적 : 충분히 살펴보지 못한 것들을 보고, 차원과 측정값들의 조합을 적절하게 바꿔가면서 관찰했는지 한 번 더 점검  분석 목표에 따른 분석 기법     분석 목표 설명 통계적 분석 기법     평균에 대한 검정, 추정 평균에 대한 모델링 T검정   비율에 대한 검정, 추정 비율에 대한 모델링 직접확률계산, F분포   비율에 대한 검정, 추정 2개 이상 차원이 있고 하나의 측정값 -\u003e 분류 조합에 따라 측정값에 유효한 차이가 있는지 검정 카이 제곱 검정, fisher의 직접 확률 검정, 멕네마 검정, 잔차 분석   상관관계 강도 도출 독립적으로 움직이는 변수들 사이 관계의 강도를 상관계수로 나타냄 상관분석   선형/비선형 인과관계의 형태, 강도 추출 독립적으로 움직이는 변수들 사이 관계의 강도를 상관계수로 나타냄 회귀분석,로지스틱, 판별분석   요인들 사이의 관계와 핵심 요인 선별 변화 요인이 되는 값들이 3개라 할때 어떤 것이 측정값에 가장 영향을 미치는지, 다른 차원의 영향력과 어느 정도 겹치는지 분석 요인분석,주성분 분석   대상들을 여러 기준으로 분류, 다차원 공간 배치 차원들의 값 기준으로 측정값들 사이 거리를 계산해 그룹을 짓고 다차원 공간에 측정값 배치 군집분석, 다차원척도법(MDS)   패턴이 비슷한 측정값과 그젛지 않은 측정값 분류 답변들의 패턴에 따라 비슷한 답변을 한 응답자와 그렇지 않은 응답자 분류 대응분석   흐름에 따라 변하는 데이터 분석 모델 도출 추세요인, 계절요인, 순환요인, 불규칙요인으로 분해해 모델을 만들어 미래 예측 시계열분석     차원이 많거나 불연속 데이터가 많은 경우 통계적 분석법 활용 통계적 분석 기법의 결과물 : 구체적 계수, 설명계수, 그래프, 걸러진 변수 시각적 도구와 통계적 도구는 상보적 관계  분석과 시각화 도구  회귀분석에서 적합한 함수식을 찾는데 보조도구로서 사용됨 회귀분석을 통해 인과관계를 살펴볼 수도 있고 전체 형태의 추세를 통해 미래 또는 축 상의 다음 측정값에 대한 예측을 할 수도 있다. 만약 시각화 해보지 않으면 도출된 예측값만으로만 보면 현실적인 가정 및 조건을 놓칠 수 있다. 통계적 분석 기법과 시각적 분석 기법은 밀접한 관계임  지표 설정과 분석  지표의 기본 개념   KPI : 기업에서 업무성과 평가 목표설정 등의 활동에 활용, 세부적인 활동 결과물 추진 정도나 수준을 측정하고 평가 도출되는 결과값을 지표로 활용할 수 있다. 의미가 있고 직관적으로 이해되는 수치들을 지표로 만드는 것이 가치가 있음  지표의 기본 구조   지표는 인사이트를 커뮤니케이션에 활용, 분석할 때도 유용 관계를 지표로 축약해 표현하면 다른 관계를 살펴보는 기준으로 삼기 편해짐  지표 활용시 주의점   단위를 잘 살펴야 함 시각화 도구에 적용할 때 적적하게 단위가 표현될 수 있는지 체크해야 함. 척도와 관련된 문제는 없는지 봐야함 지표로 분석할 때 다른 변수들이 이 지표와 어떤 관계에 있는지 봐야한다. 지표가 통계적 모델을 만들 때 포함된다면, 모델 설명력이 과대평가될 수 있다. 요인 분석 : 지표가 지표를 만든 다른 요인들과 상당 부분 설명력이 겹치는지 확인  ","description":"","tags":null,"title":"ADP 공부하기 9","uri":"/posts/certificate/2020-01-31-adp_study9/"},{"categories":["Certificate"],"content":"분산 컴퓨팅 기술 MapReduce 개념\n 구글에서 분산 병렬 컴퓨팅을 이용하여 2004년 논문에서 공개됨 분할정복 방식으로 대용량 데이터를 병렬로 처리할 수 있는 모델  분할정복 : 성질이 같은 여러 부분으로 나눠 해결한 뒤 원래 문제의 해를 구함   c++,JAVA 적용, 아파치 하둡의 Hadoop MapReduce가 동일한 기능 클라이언트의 작업 단위는 맵리듀스 잡 map, reuce task로 나뉨 map task 하나가 1개의 블록을 대상으로 연산, 사용자가 지정한 개수에 해당하는 reduce task들이 받아 정렬 및 필터링 작업을 거침  구글 맵리듀스  복잡성을 추상화시켜 핵심 기능 구현에만 집중하게 함 map에는 key와 value 쌍들을 입력으로 받음 map함수를 거치면서 다수의 새로운 key, value로 변환 reduce로 전동됨. 이 과정에서 shuffle과 group by 정렬 과정을 거쳐 reduce의 입력 레코드로 들어가게 되는데 형식은 key, value의 리스트다 reduce 함수를 통해 최종 output 산출, 사용자는 map과 reduce 함수만 작성  실행 과정  마스터는 입력 데이터소스를 가지고 스케줄링함 각 split이 map 프로세스들의 할당 단위, split은 64~128mb(블록 단위), split 수만큼 map task들이 워커로부터 fork됨 output값은 partitionar라는 reduce번호를 할당해 주는 클래스를 통해 보낼지 정함.정해지지 않으면 동일한 key들은 같은 reduce로 배정 = map단계가 끝나면 원격의 reduce 워커들이 자기에 할당된 map의 중간 값들을 네트워크로 가져, 사용자의 reduce로직을 통해 산출물 보통 reduce의 개수는 map의 개수보다 적고, map의 중간 데이터 사이즈에 따라 성능이 좌우됨 적합한 경우 : 분산grep이나 빈도수 계산 등의 작업  map단계를 거치며 데이터 사이즈가 크게 줄어들고 줄어든 크기만큼 reduce의 오버헤드도 줄어듬 = 적합하지 않은 경우 : 정렬과 같은 작업 사이즈 줄지 않고 오버헤드에 따라 수행 성능이 저하    폴트톨러런스  각 프로세스에서는 master에게 task진행 상태를 주기적으로 보냄 마스터는 워커들의 task 상태 정보를 가지고 있다가 특정 워커가 진행하지 않고 정보를 받지 못하면 task에 문제가 있다고 판정 복구를 할 때 map, reduce task들이 죽은 경우 해당 task가 데이터 정보만 다른 워커에게 전하면 새로운 task를 실행하면 됨. mapreduce는 shared nothing 아키텍쳐  Hadoop MapReduce  아파치 오픈소스 프로젝트, java로 구현 HDFS와 Hadoop MapReduce가 하둡의 핵심 구성요소  아키텍쳐  하둡은 데몬 관점에서 4개의 구성 요소를 가짐     구분 설명     네임노드 하둡을 이루는 필수적인 데몬, 마스터 역할 수행   데이터노드 분산 파일 시스템의 데몬, 데이터 입출력에 대한 처리 수행   잡트래커 시스템에서 job이라는 작업을 관리하는 마스터에 해당   태스크트래커 작업을 수행하는 워커 데몬, 슬레이브에 해당(각 노드에 1개의 태스크 트래커)     클라이언트에서 잡이라 부르는 하둡 작업을 실행 -\u003e 환경 정보들이 jobtracker에 전송 jobtracker는 작업을 다수의 task로 쪼개고 데이터 지역성을 보장하기 위해 task를 어떤 tasktracker에게 보낼지를 감안해 내부적으로 스케줄링해 큐에 저장 이 때 task는 맵퍼나 리듀서가 수행하는 단위 작업 tasktracker는 jobtracker에게 3초에 한 번씩 주기적으로 하트비트 보냄 tasktracker에서 하트비트를 보내면 jobtracker는 할당된 task가 있는지 큐에서 확인 후, 있으면 response메세지에 task정보를 실어 tasktracker에 보냄 tasktracker는 메시지의 내용을 분석해 프로세스를 fork함  실행 절차  스플릿 : 파일 스플릿 생성, 파일 스플릿 하나당 map task 하나씩 생성 맵 : 각 split에 대해 레코드 단위로 map함수 적용, key-value쌍 생성 컴바인 : 리듀스와 동일한 프로그램 적용, 데이터의 크기를 줄임 파티션 : key를 기준으로 데이터를 디스크에 분할 저장, 정렬 수행, 분할된 파일들은 각각 다른 reduce task에 저장 셔플 : 맵퍼들의 결과 파일을 각 리듀서에 할당, 할당된 파일을 로컬 파일 시스템으로 복사 정렬 : 병합 정렬 방식으로 맵퍼의 결과 파일을 key기준으로 정렬 리듀스 : 리듀스 함수 적용   기본적으로 output format은 key,value를 탭으로 구분하며, mapred.textoutputformat.separator 속성을 사용해 구분자를 원하는 문자로 변경할 수 있음 대표적 예제인 WordCount  하둡의 성능, 사용현황  sort : map에서 reduce로 넘어가는 과정에서 항상 발생하는 내부적 프로세스 sort 작업이 데이터가 커질수록 선형적으로 증가 구성 서버를 늘린다고 처리 시간이 줄어들진 않음, 플랫폼이 자체적으로 선형 확장성을 가지고 있어야함 sort는 플랫폼의 성능과 확장성을 동시에 측정할 수 있음     구분 설명     야후 -하둡 프로젝트의 주요 후원자-4만대 이상의 컴퓨터에 하둡 설치, 가장 큰 클러스터는 약 4,500만개의 노드로 구성   야후의 WebMap - 야후의 대표적 그래프 기반 검색 엔진-모든 edge 및 링크 정보를 계산해 결과를 다양한 애플리케이션에서 사용할 수 있도록 해주는 거대한 그래프-100개 이상의 MapReduce 작업들을 체인 형태로 묶어 실행, 압출해서 300TB가 나올 정도를 다루고 있음   국내 - NHN과 다음 등의 포털에서 하둡 사용-삼성SDS, SK등의 IT회사에서 하둡 활용    ---\r병렬 쿼리 시스템 개요\n 일부 사용자들에게는 mapreduce가 쉽지 않음 병렬 처리 시스템 개발, 구글의 sawzall, 야후의 pig 등  구글 Sawzall  MapReduce를 추상화한 최초의 스크립트 형태 병렬 쿼리 언어 MapReduce 생산성을 높임, 쉬운 병렬 프로그래밍 Pig나 Hive도 Sawzall와 유사  아파치 Pig  Hadoop MapReduce 위에서 동작하는 추상화된 병렬 처리 언어, 아파치의 서브 프로젝트 전체 MapReduce작업의 약 30%에 Pig사용  개발 배경\n 실제 대부분의 업무가 한 번의 MapReduce로 끝나진 않음 또한 MapReduce를 이용하는 개발자들이 유사한 알고리즘을 중복 개발하는 경우가 많지만 의미 파악이 어려워 공유는 잘 이루어지지 않음 의미적으로 SQL과 비슷하지만 새로운 언어인 Pig를 정의  사용\n MapReduce는 무공유 구조 -\u003e join연산을 매우 복잡하게 처리(400라인) Pig를 사용하면 10라인으로 가능 검색 인프라, 광고 연관성 분석, 사용자 의도 분석, 검색엔진 쿼리 분석, 호프만 plsi 등 다양한 분야에서 이용  아파치 Hive = 페이스북에서 개발, pig와 마찬가지로 하둡 플랫폼 위에서 동작\n SQL기반 언어와 JDBC지원, Hadoop-Streaming을 쿼리 내부에 삽입해 사용 맵리듀스의 모든 기능을 지원  개발 배경\n 페이스북은 초기에 DBMS기반의 시스템을 운영했으나 데이터가 수백TB규모로 늘어나 관리 및 비용 절감의 필요성 느낌 DBMS를 하둡으로 교체하는 과정에서 필요한 기능들을 하나씩 구현하면서 만들어짐  아키텍쳐\n Metastore는 raw file들의 콘텐츠를 테이블 내 칼럼처럼 구조화된 형태로 관리할 수 있게 해주는 스키마 구조 별도의 DBMS를 설정하지 않으면 Embedded Derby를 기본 DB로 사용 앞 단에는 커멘트 라인 인터페이스(CLI)가 있는데 이걸로 SQL쿼리 사용 파서에서 쿼리를 받아 구문 분석을 하고 Metastore에서 정보를 참조해 Execution Plan을 만든다. Execution Engine은 하둡의 jobtracker와 네임 노드와 통신을 담당하는 창구 역할을 하면서 Mapreduce작업을 실행하고 파일을 관리함 Serde라는 것은 Serilizer와 Deserializer의 줄임말, 테이블의 로우나 칼럶의 구분자 등 저장 포멧을 정의하는 컴포넌트  하이브의 언어모델    DDL DML Query     - 테이블 생성,삭제, 변경 명령-테이블 스키마 변경-테이블, 스키마 조회 -로컬에서 DFS로 데이터 로드-쿼리 결과를 테이블이나 로컬파일 시스템, DFS에 저장 SELECT, GROUP BY, SORT BY, JOINS, SAMPLING, TRANSFORM, SUB QUERIES, UNION     \rSQL on 하둡 개요\n 실시간 처리라는 측면에서 하둡의 제약사항을 극복하기 위한 시도, 실시간 SQL질의 분석 기술 대화형식의 SQL질의를 통해 처리하고 분석  임팔라  SQL on 하둡 기술 중 먼저 대중에게 공개된 기술 분석과 트랜잭션 처리를 모두 지원 하둡과 Hbase에 저장된 데이터를 대상으로 SQL질의를 할 수 있다. 자바 대신 C++사용, 맵리듀스를 사용하지 않고 실행 중 최적화된 코드를 생성해 데이터 처리  임팔라 구성요소    구분 설명     클라이언트 ODBC/JDBC 클라이언트, 임팔라쉘 등에 해당, 임팔라에 접속해 테이블 관리, 데이터 조회 등의 작업 수행   메타스토어 임팔라로 분석할 대상 데이터들의 정보 관리, 하이브의 메타데이터를 같이 사용   임팔라 데몬 시스템에서는 ImpalaD로 표시되며 클라이언트 SQL 질의를 받아서 읽기/쓰기 작업 수행, 질의 실행계획기, 질의 코디네이터, 질의 실행엔진으로 구성   스테이트 스토어 데몬들의 상태 체크하고 건강정보를 관리하는 데몬, 장애가 생기면 다른 데몬들에게 알려서 장애가 발생한 데몬에 질의가 가지 않도록 함   스토리지 분석할 데이터의 저장소, 현재는 Hbase, HDFS 지원    임팔라 동작 방식  모든 노드에 임팔라 데몬이 구동되고 사용자는 구동된 임의의 노드에 JDBC,ODBC, 임팔라쉘을 이용해 질의를 요청할 수 있다. 사용자의 질의는 데이터의 지역성을 고려해 노드 전체로 분산되어 수행 질의 요청을 받은 코디네이터 데몬은 분산되어 수행된 각 임팔라 노드들의 부분 결과를 취합해 결과값을 만들어 사용자에게 제공 실제 운영 환경에서는 라운드 로빈 방식으로 질의를 분산시켜 질의에 대해 전 노드들이 코디네이터 역할을 고르게 수행하도록 해야함  라운드 로빈 : 여러 프로세스들이 돌아가며 처리되는 스케줄링 방식    임팔라 SQL 구문 -임팔라는 기본적으로 하이브의 SQL을 이용하나 모든 하이브 SQL을 지원하는 것은 아니라 어떤 구문이 지원되는지 확인해야함\n   항목 설명     DDL CREATE TABLE/DB, ALTER TABLE, DROP DB/TABLE, SHOW DB/TABLE(조회)   DML SELECT, WHERE, GROUPBY, ORDERBY, INSERT, OVERWRITE (변경,삭제 구문은 지원 안함)   내장 함수 ABS, ACOS(코사인값 반환), DAY, FROM_UNIXTIME, IF, CASE,ASCII, CONCAT    임팔라 데이터 모델  임팔라는 하둡 분산 파일시스템에 데이터를 저장하며, 어떤 저장포맷을 사용하느냐에 따라 처리 성능이 다름     항목 설명     로우 단위 저장 -하둡의 기본 파일 포맷인 텍스트나 시퀀스 파일은 로우 단위 데이터 저장 방식-하나의 칼럼을 읽든 전체를 읽든 동일한 디스크 입출력 발생   칼럼 단위 저장 - 읽고자 하는 칼럼만큼의 디스크 입출력 발생 -\u003e 성능 개선(전체 칼럼을 읽을 땐 개선X)-처리 시간이 로우보다 적게 걸림, 다만 파일이 처음부터 칼럼 파일 포맷을 사용하지 않았을 때 파일 포맷 변경 작업을 해주어야 함칼럼 단위의 파일 저장 포맷인 RCFILE을 사용할 경우, 디스크 입출력 양을 현저하게 줄일 수 있다.    \n클라우드 인프라 기술 클라우드컴퓨팅  가상화 자원들을 인터넷으로 서비스하는 기술 Iaas, SaaS, PaaS의 3유형 Iaas : 네트워크 장비, 서버와 스토리지 같은 IT인프라 자원을 빌려줌 SaaS : 소프트웨어를 웹에서 사용할 수 있게 함 PaaS : 애플리케이션이나 소프트웨어 개발 및 구현 시 필요한 플랫폼 제공 VMware, Xen, KVM등과 같은 서버 가상화 기술은 IaaS에 주로 활용 아마존은 S3, EC2 환경을 제공함으로써 플랫폼을 위한 클라우드 서비스르 최초로 실현, AWS의 EMR은 하둡을 온디맨드로 이용할 수 있는 클라우드 서비스  서버 가상화의 개념 및 특징 정의 : 물리적인 서버와 운영 체제 사이에 적절한 계층을 추가해 사용자에게 물리적인 자원은 숨기고 논리적인 자원만을 보여줌 특징\n 서버 가상화는 하나의 서버에서 여러 애플리케이션, 미들웨어, 운영체제들이 서로 영향을 미치지 않으면서 동시에 사용하도록 함. 서버 가상화를 가능하게 하는 기술은 다양하고 메인프레임, 유닉스서버, X86서버 등에 따라 다른 기술,분류체계가 사용됨  서버 가상화 기술의 효과  가상 머신 사이의 데이터 보호  다른 가상머신들 사이의 접속은 정상적인 네트워크 접속만 허용   예측하지 못한 장애로부터 보호  장애가 다른 가상머신에는 전혀 영향을 미치지 않음   공유자원에 대한 강제 사용의 거부  할당된 자원 이상을 가져가는 것을 차단함, 다른 가상머신에 할당된 자원 부족 현상을 차단함   서버 통합  동일한 데이터 센터의 물리적 자원을 이용하면서 더 많은 서버를 운영할 수 있다.   자원할당에 대한 증가된 유연성  전체 시스템 자원을 재배치해 자원 활용도 극대화   테스팅  새로운 서버를 추가하지 않아도 테스트 환경을 구성할 수 있음.   정확하고 안전한 서버 사이징  사이징 예측이 불확실한 서버르 구성할 때도 일단 확보된 리소스를 이용해 할당 후 쉽게 추가로 할당할 수 있다.   시스템 관리  마이그레이션 기능을 이용할 경우 운영 중인 가상머신의 중지 없이 다른 물리적 서버로 이동시킬 수 있다. 하드웨어 장애, 로드 밸런싱, 업그레이드 업무를 쉽게 수행할 수 있음    CPU 가상화 하이퍼 바이저의 개념 및 특징  물리적 서버 위의 가상화 레이어를 통해 필요한 하드웨어 환경을 가상으로 만듬 하이퍼 바이저 : 호스트 컴퓨터에서 다수의 운영체제를 동시에 실행하기 위한 논리적 플랫폼 일반적인 가상머신은 하이퍼바이저(VMM) X86 계열에선 소프트웨어 기반  기능  하드웨어 환경 에뮬레이션 실행환경 관리 시스템 자원 할당 소프트웨어 스택 보존  하이퍼바이저 관련 기술 분류  플랫폼별 분류 X86 : VMware,MS Virtuall Server, Xen 유닉스 계열 : IBM - POWER Hypervisor 메인 프레임 계열 : z/VM, PR/SM 위치와 기능에 따른 분류   가상화를 제공하는 하이퍼바이저가 물리적 하드웨어 또는 호스트 운영체제와 관계에서 어디 위치하는지에 따라 베어메탈, 호스트 기반으로 나뉨 베어메탈은 하드웨어와 호스트 운영체제 사이에 위치, 호스트는 호스트 운영체제와 게스트 운영체제 사이에 위치 베어메탈은 반가상화, 완전 가상화로 구분  privileged 명령어 처리 방법에 따른 분류   최근에는 새로운 가상화 방법이 계속 나와 정확한 분류가 어려움 x86 운영체제는 모든 하드웨어에 대한 제어 소유권을 가지고 있다는 가정 아래 하드웨어에 직접명령을 수행하는 방식으로 디자인됨 x86 아키텍쳐는 Ring 0,1,2,3 등 4개 레벨로 구성, 운영체제는 0레벨, 사용자는 3레벨 운영체제가 3레벨로 수행될 경우 복잡한 문제 발생 3이 수행된 운영체제에서 0 수준 명령을 호출하면 이를 0 수준의 명령어로 다시 변환해 실행해야함 반가상화, 완전 가상화 용어도 privileged명령어를 어떻게 처리하느냐를 기준으로 분류  가상화 방식 분류  완전 가상화   하이퍼바이저보다 우선 순위가 낮은 가상머신에서는 실행되지 않는 privileged명령어에 대해 trap을 발생시켜 하이퍼바이저에서 실행하는 방식 VMware ESX Server, MS Virtual Server 최근 Xen에서 Intel vt-X, AMD-V환경에서 지원 장점  CPU뿐만 아니라 모든 자원을 하이퍼바이저가 직접 제어,관리하기 때문에 어떤 운영체제라도 수정하지 않고 설치 가능 MS윈도우와 같은 게스트 OS가 변경되지 않은 상태로 실행 가능   단점  하이퍼바이저가 직접 자원 제어 -\u003e 성능에 영향 자원들이 하이퍼 바이저에 너무 밀접하게 연관되어 있어 동적변경 작업이 단일 서버내에는 어려움 동적변경을 하기 위해서는 VMware의 VMotion과 같은 솔루션의 도움을 받아야 함 Para Virtualization에 비해 속도가 느림    하드웨어 지원 완전 가상화   메모리와 CPU 등 하드웨어에 명령을 내릴 수 있는 반가상화 수준의 성능을 발휘 CPU에 Ring-1 계층 추가, 하이퍼바이저가 ring-1에서 수행되고 가성머신의 os는 ring-0에서 수행되어 previleged명령어에 대한 변환과정이 필요없다. 빠른 성능, 윈도우2008의 Hyper-V는 반드시 가상화 지원 CPU만 써야함 인텔에서는 CPU사용률이 높아져 서버 통합을 목적으로 할 경우 비효율적이라고 제시함. 인텔에서 반가상화와 하드웨어 지원 완전 가상화를 모두 사용하는 하이브리드 가상화 제시 Xen의 하이브리드 가상화의 경우 명령어의 종류에 따라 반가상화와 완전 가상화를 선택함  반가상화   previleged 명령어를 게스트 운영체제에서 hypercall로 하이퍼바이저에 전달하고 하이퍼바이저는 hypercall에 대해 previleged 레벨에 상관없이 하드웨어로 명령 수행 hypercall : 게스트 os에서 요청하면 하이퍼바이저에서 바로 명령을 실행하는 call hypercall을 요청하기 위해서는 게스트os 일부분이 수정되어야 함(Xen에서 20% 커널이 수정됨) 반가상화 기반에서는 CPU와 메모리 자원의 동적 변경이 서비스 중단 없이 이루어질 수 있고 완전 가상화보다 성능이 뛰어남. 속도는 빠르나 커널을 변경해야하고 완전 가상화는 커널 변경은 없다. VMware같은 상용 솔루션은 완전 가상화와 반가상황의 단점을 보완해 뚜렷한 차이가 없음 VMI라는 표준 인터페이스를 제시해 모든 게스트OS를 지원하는 체계로 반가상화 지원 VMI는 아직 정식은 아니지만 리눅스 진영에서 도입하려는 움직임이 있음.  Monolithic vs Microkernel   드라이버가 어느 계층에 있느냐로 나뉨     Monolithic 방식 Microkernel 방식     -가상머신이 I/O를 위해 하드웨어에 접근할 때 드라이버를 하이퍼바이저 계층에서 모두 갖고 있는 방식\n-VMware의 경우 하이퍼바이저가 드라이버를 갖고 있고 모든 I/O요청은 하이퍼바이저가 수행\n-성능은 조금 향상될 수 있지만 많은 코드를 가져서 장애가 발생할 가능성이 높다. -I/O를 위해 하드웨어에 접근할 때 드라이버를 각 가상머신에서 가지는 방식\n-Xen에서 하이퍼바이저는 드라이버가 없고 호스트OS가 가지고 있음. I/O 요청을 위해선 호스트OS를 거쳐야 함\n-게스트와 호스트 OS는 서로 격리되어 있어 하이퍼바이저를 이용해 요청을 주고받음\n속도는 느리지만 하이퍼바이저 계층이 간단하여 하이퍼바이저 변경이 필요없고 장애 확률이 낮다.    하이퍼바이저 기반 가상화 기술 비교    구분 완전 가상화\nCPU 기술 이용X 완전 가상화\nCPU 기술 이용 반가상화     사용기술 바이너리 변환\nDirect Execution Privileged Instruction은 Ring-1로 처리 hypercall가능하게 게스트os변경\n호환성 안좋음   게스트os\n반응/호환성 게스트os 변경 없음\n호환성 뛰어남 게스트os 변경 없음\n호환성 뛰어남 hypercall가능하게 게스트os변경\n호환성 안좋음   성능 좋음 Fair\n(바이너리 변환 방식의 성능에 근접) 특정 경우에 좋음   성능 VMware, Microsoft, Parllels VMware,Microsoft,Parllels,Xen VMware,Xen    호스트 기반 가상화   완전한 os가 설치되어 하이퍼바이저가 호스트 os위에 탑재되는 방식 제약 사항이 많고 단일 os의 취약성이 있다.(신뢰성 문제) VMware, Workstation, Microsoft Virtual PC 테스트용 환경, 최근 사용x  컨테이너 기반 가상화   운영체제만을 가상화한 방식 가상화 지원계층을 하이퍼바이저가 아닌 가상 운영환경이라 부름 Virtuozzo, Solaris Containers, Linux-VServer 장점  하이퍼바이저 가상화 방식에 비해 훨씬 적게 가상화 가상화 수준이 낮아 빠른 성능 한 대의 서버에서 더 많은 컨테이너를 실행할 수 있음   단점  격리 수준이 낮아 다른 가상 운영체제가 영향받음 보안 취약성에 의해 모든 가상 os에 문제가 생길 수 있음 호스트os를 공유하기 때문에 호스트 os문제가 전체 가상 os에도 영향 미침    메모리 가상화 : VMware 기법 개념\n 가상의 공간을 만들어주는 프로그램 물리주소 : 0부터 실제 물리적인 메모리 크기까지를 나타냄 가상주소 : 하나의 프로세스가 가리킬 수 있는 최대 크기(32비트에서 4GB) 프로그램에서 주소는 가상주소값 -\u003e 가상주소값 위치와 물리적 주소값 위치 매핑 과정 필요 TLB : 매핑 연산을 하드웨어적으로 도와주는 것 하이퍼바이저 내에 Shadow Page Table을 따로 두어 중간 변환 과정을 가로챔 모든 가상머신들이 자신만의 메모리 주소 공간을 갖도록 함 메모리 할당 문제를 해결해야함  가상머신 메모리 할당의 문제 해결 방법  Memory ballooning  VMkernel은 가상머신의 메모리 영역을 빈 값으로 강제로 채워 가상머신os가 자체적으로 swapping하도록 함 물리적인 메모리가 채워지고 있다는 것을 감지한 가상머신os는 swap파일에 메모리 영역을 page out시키고 자리를 비우게 됨. 하이퍼바이저는 page out된 메모리 영역을 다른 가상머신에 할당함   Transparent page sharing  하나의 물리적 머신에 여러 개의 가상머신이 운영되는 경우 각 가상머신에 할당된 메모리 중 동일한 내용을 담고 있는 페이지는 물리적 메모리 영역에 하나만 존재시키고 모든 가상머신이 공유하도록 함   Memory Overcommitment  2GB 메모리를 가진 물리적 장비에 512GB를 Mininum reserved를 가질 수 있는 가상머신 5개를 수행할 수 있음 앞의 2가지 기법으로 가능하지만 모든 가상머신이 메모리 사용이 많은 업무를 수행하는 경우면 심각한 성능저하 현상이 발생할 수 있어 권장하진 않음.    I/O가상화  I/O병목현상이 가장 문제가 됨 -\u003e I/O자원의 공유 및 파티셔닝이 필요 또한 가상머신 간에도 통신이 이루어져야 함. 이를 위해 다양한 기술이 사용  이더넷 : IEEE가 표준 사양으로 택한 LAN에 사용되는 모델로 근거리 통신망 하드웨어, 프로토콜, 케이블 표준      가상 이더넷\n 가상 이더넷은 물리적으로 존재하지 않는 자원을 만들어내는 에뮬레이션 기능 사용 각 가상머신 사이에 네트워크 어댑터 없이도 메모리 버스를 통해 고속 및 고효율 통신이 가능 가상 LAN기술을 기반으로 한 네트워크 파티션도 가능하게 함, 각 가상 LAN 사이에는 통신을 할 수 없음 사용자들은 별도의 물리적 어댑터와 케이블을 사용하지 않고도 네트워크 이중화, 네트워크 안전성 단절 등의 효과를 얻을 수 있음    공유 이더넷 어댑터\n 여러 개의 가상머신이 물리적 네트워크 카드를 공유할 수 있게 하고 외부 네트워크와 통신이 가능하게 함 가상 머신 개수보다 물리적 어댑터 개수가 적은 경우 가상머신들이 물리적 이더넷 어댑터를 공유할 수 있게 해줌 하나 자원을 여러 가상 머신이 공유해 병목현상을 피할 수 없음 최근에는 네트워크 어댑터 내에서 가상화를 지원하게 함    가상 디스크 어댑터\n 파이버 채널 어댑터와 같은 I/O어댑터의 부족    가상화된 환경에서 가상 디스크를 이용해 가상머신이 디스크 자원을 획득하는 방법\n   내장 디스크 외장 디스크     -가상 I/O레이어가 내장 디스크를 소유하고 있고 이 것을 논리적 디스크 드라이브로 나눈다.\n-나누어진 드라이버는 LUN으로 각 파티션에 가상 디스크 어댑터를 통해 분배됨\n-이렇게 획득한 논리적 디스크 자원을 물리적 자원처럼 인식 -먼저 가상 I/O레이어가 파이버 채널 어댑터를 통해서 외장 디스크의 LUN을 획득\n-내장 디스크와 달리 가상 I/O레이어가 바로 각 가상머신에 가상 디스크 어댑터를 통해 분배\n-가상 I/O레이어를 통해 제공도니 논리적 디스크 볼륨은 이를 이용하는 다른 가상머신에게는 SCSI 디스크로 나타냄    ","description":"","tags":null,"title":"ADP 공부하기 8","uri":"/posts/certificate/2020-01-30-adp_study8/"},{"categories":["Certificate"],"content":"데이터 처리 기술 분산 파일 시스템 개요  저장 기술은 분산 파일시스템, 클러스터, DB, NOSQL로 구분됨 사용자 중심의 인터넷 서비스와 유비쿼터스 컴퓨팅 환경은 대규모 클러스터 시스템 플랫폼의 필요성을 부각시킴. 최근에는 파일의 메타데이터를 관리하는 전용 서버를 가지고 있는 ‘비대칭형 클러스터 파일 시스템’이 활발히 개발  구글 파일 시스템 (GFS) 개념\n GFS는 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 파일 시스템 파일을 고정된 크기(64MB)의 CHUNK들로 나누고 각 CHUNK에 대한 여러 개의 복제본과 CHUNK를 청크서버에 분산저장함 기본 64MB로 정하고 해시 테이블 구조 사용 -\u003e 효율적인 메타데이터 처리 지원 CHUNK는 마스터에 의해 생성/삭제, 유일한 식별자에 의해 구분됨  GFS 설계의가정  서버의 고장이 빈번히 발생할 수 있다 가정 작업 부하는 주로 연속적으로 많은 데이터를 읽는 연산이거나 임의의 영역에서 적은 데이터를 읽는 연산에서 발생 쓰기 연산은 주로 순차적으로 이루어지며, 갱신은 드물게 이루어짐 동기화 오버헤드를 최소화할 수 있는 방법이 요구됨 낮은 응답 지연시간보다 높은 처리율 중요  GFS 구성요소  여러 클라이언트, 하나의 마스터 청크서버들     클라이언트 - 파일에 대한 읽기 쓰기 동작을 요청하는 애플리캐이션, POSIX인터페이스를 지원하지 않고 자체 인터페이스 지원- 원자적인 데이터 추가 연산을 지원하기 위한 인터페이스 지원     마스터 - 단일 마스터 구조, 이름 공간, 파일과 CHUNK의 매핑정보, 청크서버들의 위처정보 등의 모든 메타데이터를 메모리상에서 관리- 청크서버의 하트비트 메시지를 이용해 CHUNK의 상태에 따라 CHUNK를 재복제하거나 재분산하는 것 같은 회복동작 수행- 하나의 청크서버를 primary로 지정해 복제본의 갱신 연산을 일관되게 처리 가능- 마스터에 대한 장애 처리와 회복을 위해 이름 공간과 매핑 변경 연산을 로깅하고 마스터의 상태를 여러 셰도 마스터에 복제   청크서버 - 로컬 디스크에 CHUNK 저장,관리 클라이언트로부터 CHUNK 입출력 요청 처리- 하트비트 메세지를 통해 청크서버의 상태에 대한 정보를 주기적으로 마스터에게 전달    GFS에서 파일 읽기  클라이언트는 파일에 접근하기 위해 마스터로부터 해당 파일의 CHUNK가 저장된 CHUNK 서버의 위치와 핸들을 받아온 뒤, 직접 청크서버에 파일 데이터를 요청  하둡 분산 파일 시스템(HDFS) 개념 : 아파치 너치의 파일 시스템으로 개발, 클로닝 프로젝트\n 마스터와 유사한 네임노드, 청크서버와 유사한 데이터 노드 HDFS에서 파일 데이터는 블록 단위 , 여러 데이터 노드에 분산,복제,저장됨 파일은 한번 쓰이면 변경되지 않는다고 가정 순차적 스트리밍 방식 낮은 응답 지연시간보다 높은 처리량 중요 통신을 위해 TCP/IP에서 RPC사용  HDFS 구성 요소    네임 노드 - 파일 시스템의 이름 공간 등 HDFS 상의 모든 메타 데이터를 관리, 마스터 역할- 파일이 블록 단위로 나누어져 있고 어떤 노드에 특정 블록이 있는지 시스템 전반 상태를 모니터링- 데이터 저장, 애플리케이션 실행은 하지 않음-클라이언트로부터 파일 접근 요청 처리-데이터노드로부터 하트비트를 받아 상태체크, 블록 정보를 가지고 블록 상태 체크     데이터 노드 - HDFS의 ㅡ슬레이브 노드, 데이터 입출력 요청 처리- 데이터 유실을 방지하기 위해 3중 복제 저장-파일의 체크섬 정보를 별도 저장-주기적으로 데이터노드의 상태를 나타내는 하트비트와 자신이 관리하는 블록의 목록인 blockreport를 네임노드에 전송   보조네임노드 - HDFS 상태 모니터링을 보조- 주기적으로 네임 노드의 파일 시스템 이미지를 스냅샷해 생성    \rHDFS 파일 저장 과정  64MB, 128MB 단위의 블록으로 분리 첫 번째 데이터노트로부터 세 번째 데이터노드까지 저장 완료 -\u003e 각 데이터 노드들은 순차적으로 클라이언트에게 저장이 완료되었다는 신호를 보냄 모든 블록의 저장이 완료될 때까지 반복 모든 블록의 저장이 완료되면 네임노드는 블록들이 저장된 데이터노드의 주소(메타데이터) 저장  HDFS 파일 읽기 과정  클라이언트가 읽고자 하는 파일의 정보를 네임노드에게 요청 네임노드는 모든 블록의 목록과 블록이 저장된 데이터 노드의 위치를 클라이언트에 반환  러스터 개념\n 클러스터 파일 시스템에서 개발한 객체 기반 클러스터 파일 시스템 고속 네트워크 연결 클라이언트 파일 시스템, 메타데이터 서버, 객체 저장서버들로 구성 계층화된 모듈 구조, TCP/IP, 인피니밴드, 미리넷과 같은 네트워크 지원  구성요소    클라이언트 파일시스템 - 리눅스 VFS에서 설치할 수 있는 파일 시스템- 메타데이터 서버와 객체 저장 서버들가 통신하며 클라이언트 응용에 파일 시스템 인터페이스 제공     메타데이터 서버 - 파일 시스템의 이름 공간과 파일에 대한 메타데이터 분리   객체 저장 서버 - 파일 데이터 저장, 클라이언트로부터 객체 입출력 요청 처리- 데이터는 세그먼트라는 작은 단위로 분할해 복수의 디스크 장치에 분산시키는 ‘스트라이핑 방식’    구동 방식\n 라이트백 캐시 지원 클라이언트에서 메타데이터 변경에 대한 갱신 레코드 생성, 나중에 메타데이터 서버에 전달 메타데이터서버는 전달된 갱신 레코드를 재수행해 변경된 메타데이터를 반영 클라이언트에서 라이트백 캐시 지원, 메타데이터 서버에서 메타데이터를 처리하는 방식을 적용 동시 접근이 적으면 클라이언트 캐시를 위용한 라이트백 캐시 사용, 많으면 클라이언트 캐시 사용 -\u003e 오버헤드 줄임 동시성 제어를 위해 별도의 잠금 사용 인텐트 기반 잠금 프로토콜 : 네트워크 트래픽 최소화 위해 잠금 요청 시 접근 의도를 같이 전달  라이트백 캐시 : 데이터를 캐시에만 저장하고 어쩔 수 없이 캐시영역에서 밀려나는 경우 하위저장소에 저장    데이터베이스 클러스터 개념\n 하나의 DB를 여러 개의 서버 상에 구축 파티셔닝 : DB를 여러 부분 분할, 분할된 요소는 파티션 각 파티션은 트랜잭션 수행 데이터를 통합할 때 성능과 가용성의 향상을 위해 데이터베이스 차원의 파티셔닝 또는 클러스터링 이용  효과    병렬처리 파티션 사이의 병렬 처리를 통한 빠른 데이터 검색 및 성능     고가용성 특정 파티션에서 장애가 발생해도 서비스가 중단되지 않음   성능 향상 성능의 선형적인 증가효과    데이터 베이스 클러스터의 구분 형태에 따라 단일 서버 내 파티셔닝과 다중 서버 사이의 파티셔닝으로 구분 리스크 공유 관점에서는 공유 디스크, 무공유 디스크로 구분\n무공유 디스크\n 무공유 클러스터에서 각 DB 인스턴스는 자신이 관리하는 파일을 자신의 로컬 디스크에 저장, 노드 간 이 파일들은 공유X 각 인스턴스나 노드는 완전히 분리된 데이터 서브 집합에 대한 소유권을 가짐, 소유권을 가진 인스턴스가 처리 노드가 처리 요청을 받으면 데이터를 가진 노드에 신호를 보냄 Oracle RAC을 제외한 대부분 DB 클러스터가 무공유 방식 장점 : 노드 확장 제한X 단점 : 장애가 발생할 경우를 대비해 별도의 폴트톨러런스 구성  폴트 톨러런스 : 고장이 발생해도 일부를 유지하는 기술    공유 디스크\n 파일은 논리적으로 모든 DB 인스턴스 노드들은 논리적으로 공유, 각 인스턴스는 모든 데이터에 접근할수 있다 공유하려면 SAN과 같은 네트워크가 있어야함 모든 노드가 데이터를 수정할 수 있음 -\u003e 별도의 커뮤니케이션 채널 필요 장점 : 높은 수준의 폴트톨러런스 제공 -\u003e 하나의 노드만 살아도 서비스 가능 단점 : 클러스터가 커지면 디스크 영역에서 병목현상 발생  데이터 베이스 클러스터 종류   Oracle RAC DB 서버\n 공유 클러스터, 모든 노드에서 실행 특정 노드가 데이터를 소유하는 개념이 없다 파티셔닝 필요X, 하지만 성능 향상을 위해 하는 경우 빈번 응용 프로그램은 RAC클러스터에 연결, RAC는 클러스터 모든 노드에 로드를 고르게 분산 장점 : 가용성, 확장성, 비용 절감    IBM DB2 ICE\n 무공유 클러스터 데이터가 어느 파티션에 존재하는지 알 필요가 없다. 데이터와 사용자가 증가해도 시스템의 성능과 용량을 일정하게 유지할 수 있다. 파티셔닝 구성에 따라 성능 차이가 있다 별도의 페일오버 메커니즘 필요 -\u003e DB2를 이용해 클러스터링 구성할 때는 공유 디스크 방식 사용해 가용성 보장 장애 상황이 발생하면 다른 노드가 해당 데이터에 대한 서비스를 처리하는 방식  페일오버 : DB의 최신 버전을 백업해두어 장애가 발생했을 때 장애 극복      마이크로소프트 SQL 서버\n 연합 DB 형태 -\u003e 여러 노드로 확장 기능 독립된 서버에서 실행되는 다른 DB간 결합 수평 분할, 모든 파티션에 대해 UNION ALL로 논리적인 뷰(DPV)를 구성 마이크로 소프트 SQL 서버 구성 문제점  DBA개발자가 파티셔닝 정책에 맞게 테이블과 뷰를 생성해야됨 전역 스키마 정보가 없어서 모든 노드를 액세스해야 함 노드가 많아지거나 노드의 추가/삭제가 발생할 경우 파티션을 새로 구성해야함 페일오에 대해 별도로 파티션 구성해야함   Active-Standby 방법 사용    MySQL\n 비공유형, 메모리 기반 DB 클러스터링 지원 병렬 서버구조 확장 가능, 관리 노드, 데이터 노드, MySQL노드로 구성 관리 노드 : 클러스터 관리, 시작과 재구성 시에만 관여 데이터 노드 : 클러스터의 데이터 저장 MySQL 노드 : 클러스터 데이터에 접근을 지원 가용성을 높이기 위해 다른 노드에 데이터를 복제 복구되어 투입되어도 기존 데이터와 변경된 데이터에 대한 동기화 작업이 자동 수행 동기화 방식 복제 -\u003e 데이터 노드 간 별도의 네트워크를 구성 최근 버전에서는 디스크 기반 클러스터링, 인덱스가 생성된 칼럼은 기존과 동일하게 메모리에 유지, 생성하지 않은 칼럼은 디스크에 저장  제한 사항\n LINEAR KEY 파티셔닝만 가능 클러스터 참여 노드 수는 255로 제한, 데이터 노드는 최대 48개 문제가 발생하면 트랜잭션 이전으로 롤백해야함 여러 개 트랜잭션으로 분리해 처리하는 게 좋음 칼럼명 길이 31자,, 테이블명 길이 122자, 메타데이터 2만320개 클러스터 테이블 2만 320개, 로우 8KB, 테이블 키 32개 최대 모든 클러스터 기종은 동일해야 함. 운영 중 노드를 추가 삭제할 수 없다. 디스크 기반일 경우 tablespace $2^{32}$, tablespace당 파일 개수 $2^{16}$ 파일 크기 32GB    NoSQL 개념\n 분산 데이터베이스 기술, 비관계형 DB 관리 시스템 구조에 따라 key-value, Document, Graph, Column 모델로 구분 key, value의 형태 자료저장 스키마 없이 작동, 구조 정의 변경 없이 추가 가능 join 지원X, 대규모 수평적 확장성 대부분이 오픈 소스 구글 빅테이블, 아파치 base, 아마존 SimpleDB, 마이크로소프트 SSDS   구글 빅테이블 개념   구글의 개발, 공유 디스크 방식 모든 노드가 데이터 인덱스 파일 공유 유사한 솔루션 : Neptune  모델\n 모든 데이터는 row-key의 사전적 순서로 정렬,저장 파티션도 row-key로 이루어지고 Tablet이라 불리는 파티션은 분산된 노드에서 서비스됨 100~200MB row는 n개의 column-family를 가질 수 있고 column-key, value, time stamp형태로 데이터 저장 동일한 column-key에 대해 timestamp가 다른 여러 버전의 값이 존재할 수 있음 빅테이블 정렬 기준은 ‘rowkey + column-key + timestamp’  페일오버\n 장애가 발생할 때 Tablet을 다른 노드로 재할당, GFS에 저장된 것을 이용해 초기화 작업 수행 후 다시 서비스를 함 SPOF는 마스터다 분산 락 서비스를 제공하는 Chubby를 이용해 마스터를 모니터링 하다가 장애가 발생하면 가용한 노드가 마스터 역할을 함 Chubby는 폴트톨러런스 구조라 절대 장애 발생하지 않음 빅테이블은 별도 클러스터 구성하기 보다는 파일시스템, 맵리듀스 컴퓨팅 클러스터와 동일한 클러스터 위에 구성됨  AppEngine\n 구글 클라우드 플랫폼의 일부, 빅테이블 사용 추상 계층을 두고 API 직접 공개x, 데이터 모델도 추상화 생성되는 것이 아닌 특정 한 테이블에 대한 한 영역만 차지하게 됨 쿼리를 분석해 자동으로 인덱스 생성 인덱스가 빅테이블의 특정 테이블, 테이블 내 칼럼으로 저장됨  \rHBASE   HDFS를 기반으로 구현된 칼럼 기반 분산 데이터베이스 관계형 구조X, SQL 지원X 비구조화된 데이터에 적합, 실시간 읽기/쓰기에 용이 선형 확장 가능 구조, 복제 기능, 수평적 확장성 큰 테이블에 적합, 트랜잭션 보장, Zookeeper를 이용한 고가용성  아마존 SimpleDB   아마존의 다른 플랫폼 서비스와 같이 사용 사용자는 EC2에서 수행되는 웹 서버로 접근하고 웹 서버에서 SimpleDB의 데이터를 조회해 적절하게 가공 후 사용자에게 제공 관계형, SQL 지원 X, 전용 쿼리 언어 사용 데이터 모델은 Domain, Item, Attribute, Value로 구성, 스키마 없음     도메인 테이블과 동일한 개념, 최대 10GB 저장, 100개 도메인 최대1000GB데이터 저장 가능     Item 레코드와 동일한 개념, 독립적 객체, 1개이상 256개 이하 어트리뷰트 가짐   Attribute 칼럼과 동일한 개념, 정의할 필요가 았음\n특정 어트리뷰트에는 여러개의 값 저장 가능     한 번에 하나의 도메인에 대해서만 쿼리 수행해야 함 1:N관계의 모델을 갖는 두 개의 도메인으로부터 조회할 경우 쿼리가 여러번 수행되어야 함 다음과 같은 API 제공     CreateDomain 도메인 생성     DeleteDomain 도메인 삭제   ListDomains 모든 도메인 목록 가져옴   PutAttributes 아이템 생성후 Attributes에 값 추가   DeleteAttributes Attributes값 삭제   GetAttributes Attributes값 조회   Query 쿼리를 이용하여 조건에 맞는 아이템 조회\n5초 이내 수행되어야하고 최대 item수는 256개    마이크로소프트 SSDS   컨테이너, 엔티티로 구성 컨테이너 : 테이블과 유사한 개념, 하나의 컨테이너에 여러 종류의 엔티티 저장 가능 엔티티 : 레코드 유사 개념, 여러개의 property 가질 수 있음. property는 name-value 쌍으로 지정 정보를 하나의 컨테이너에 저장 이런 방식으로 컨테이너를 구성하면 많은 컨테이너가 생성됨, 이들은 여러 노드에 분산, 관리됨 쿼리는 하나의 컨테이너만을 대상으로 함 컨테이너 생성,삭제 엔티티의 생성, 삭제 , 조회, 쿼리 등의 API를 제공, SOAP/REST 기반 프로토콜을 지원  ","description":"","tags":null,"title":"ADP 공부하기 7","uri":"/posts/certificate/2020-01-29-adp_study7/"},{"categories":["Certificate"],"content":"데이터 처리 프로세스 데이터 통합 및 연계 기법 데이터 연계 및 통합 유형(동기화 기준)   연계 통합시 일괄(BATCH) 작업, 비동기식 근접 실시간(NRT), 또는 동기 실시간 방식 혼용\n  실시간 통합 : 관심 대상 영역 상태에 대한 빠른 파악 및 대응 가능\n  일괄 작업 : ETL기능을 통해 정기적,반복적으로 대량의 데이터를 획득해 ODS 구성, 이후 데이터 웨어하우스나 마트를 구성 후 OLAP 정형/비정형 질의를 통해 경영 분석\n  동기식 실시간 데이터 통합 : 생산 및 운송 장비 센서들로부터 데이터를 실시간으로 획득해 상태를 모니터링, 작업을 통제(Complex event Processing)\n  최근 데이터 중복을 허용하는 분산 저장 환경구성을 통해 높은 확장성을 확보하는 빅데이터 저장 인프라스트럭처의 활용과 병행 설계되는 사례도 등장\n  ETL기술은 최근 ODS,MDM 허브, 플랫폼, 하둡, 클라우드 환경 등 다양한 데이터 통합 메커니즘 지원\n  최근 비정형, 준정형 데이터 중요성 부각 -\u003e 정형 데이터로 변환은 빅데이터의 주요한 기술적 특성\n  빅데이터 기술을 사용하지 않으면 확장성과 유연성을 확보하기 어려움, 기업 IT투자를 중장기적으로 보호할 수 없음.\n  대용량의 비정형 데이터 로그  로그는 기업의 대표적 비정형 데이터 수집 시스템 : 아파치 Flume-NG,Chukwa, 페이스북 Scrive  대용량 비정형 데이터 수집 시스템 특징   초고속 수집 성능과 확장성\n 실시간으로 발생하는 대용량 데이터를 놓치지 않고 수집, 서버 수만큼 에이전트 수를 늘리는 방식으로 쉽게 확장    데이터 전송 보장 메커니즘\n 수집된 데이터는 분산 파일시스템, DB, NoSQL 등에 저장, 전송 안정성 수준 제어 가능 여러 단계를 거치는데 단계별로 신호를 주고 받아 이벤트의 유실 방지 각 방식은 성능과 안정성이라는 트레이드 오프가 존재    다양한 수집과 저장 플러그 인\n 데이터 저장소의 경우 하둡 저장 기능, NoSQL을 포함한 다양한 DB저장 플러그인 제공    인터페이스 상속을 통한 애플리케이션 확장\n 비즈니스 용도에 맞게 수정할 수 있어야 한다.     \r대규모 분산 처리 하둡  MapReduce시스템과 HDFS를 핵심 구성요소로 가짐 여러 대의 컴퓨터를 하나의 시스템인 것처럼 묶어 빅데이터를 저장 처리하는 자바 기반 오픈소스 프레임워크 수십GB에서 수십TB에 이르거나 대규모의 컴퓨팅 및 연산 작업이 필요하다면 하둡 사용 비공유 분산 아키텍처 제공  하둡 특징  선형적인 성능과 용량 확장  여러 대의 서버로 클러스터를 만들어 하둡을 구축할 때 서버의 대수에 제한 없고, 최소 5개 비공유 분산 아키텍처 시스템 서버를 추가하면 연산 기능과 저장 기능이 서버의 대수에 비례   고장 감내성  HDFS에 저장되는 데이터는 3중복제가 되어 데이터 유실 방지 맵리듀스 작업 중 장애가 생기면 장애가 발생한 특정 테스크만 재실행 가능   핵심 비즈니스 로직에 집중  맵리듀스는 맵과 리듀스라는 2개의 함수만 구현 알고리즘 및 비즈니스 로직 개발자는 분석 방식만 이해하고 목적에 맞게 간단한 코드만 작성하면 데이터의 대소에 신경 안써도됨 오직 비즈니스 로직에 집중하도록 장애에 자동복구, 확장성 및 성능도 하둡이 내부적으로 최적화함   풍부한 에코시스템  zookeeper: 서버들 간에 상호 조정이 필요한 다양한 서비스 제공 oozie : 작업을 관리하는 WORKFLOW 및 코디네이터 시스템 Hbase : HDFS 기반의 컬럼 NoSQL Pig : 복잡한 맵리듀스 프로그래밍을 대체할 Pig Latin 제공 Hive : 데이터 웨어하우스, 테이블 단위 저장, SQL쿼리 지원 Mahout : 데이터 마이닝 알고리즘을 구현한 오픈 소스 라이브러리 Hcatalog : 테이블 및 스토리지 관리 Avro : RPC와 데이터 직렬화를 지원하는 프레임 워크   RPC(Remote Procedure Call)  Chukwa : 분산 환경에서 생성된 데이터를 HDFS에 안정적으로 저장하는 플랫폼 Flume : 소스서버에 에이전트가 설치, 에이전트로부터 데이터를 전달받는 콜렉터로 구성 Scribe : 페북에서 개발된 수집 플랫폼, Chukwa와 달리 중앙집중서버로 전송 Sqoop : 대용량 데이터 전송 솔루션, HDFS, RDBMS, DW, NoSQL 등 다양한 저장소에 신속하게 전송할 수 있는 방법 제공 Hiho : Sqoop과 같은 대용량 데이터 전송 솔루션, SQL지정, JDBC 이넡페이스 지원     다양한 응용기술들이 오픈소스 프로젝트의 형태로 제공, 이를 바탕으로 하둡 에코시스템 구성 맵리듀스와 HDFS는 빅데이터 처리와 분산을 위한 기반 기술로 에코시스템의 코어 프로젝트 YARN은 맵리듀스의 단점을 극복하기 위해 하둡2.0부터 제공되는 프로젝트로 자원 관리 프레임워크 지원 Flume-NG : 데이터가 발생하는 애플리케이션 단계, 데이터 수집, 데이터 저장, 저장소 보관 단계로 이루어져 설정하지 않으면 네번째 단계는 하둡이 저장소로 사용  \r데이터 연동 개요 : DB를 대상으로 대규모 분산 처리를 하는 것은 심한 부하를 야기 -\u003e 데이터를 하둡으로 복사 후 하둡에 대규모 분산 처리를 함 이 때 데이터 연동 기능을 수행하는 대표적인 오픈 소스 솔루션이 Sqoop\nSqoop  MYSAL, PostgreSQL, 사이베이스 등 JDBC를 지원하는 대부분의 관계형 DB와의 연동 지원 일부 NOSQL DB와도 연동 가능 맵 인풋 포맷터 사용 데이터 이동을 맵리듀스로 처리, 장애 허용 능력과 병렬 처리 기능 IMPORT명령어로 RDBMS의 데이터를 HDFS로 옮기고 EXPORT명령어로 HDFS의 데이터를 RDBMS로 옮길 수 있다. 스크립트 문법을 이용해 하둡의 결과 데이터를 다시 관계형 DB로 적재할 수 있다.  대용량 질의 기술 개요 : 친숙한 SQL을 이용해 쉽게 처리하고 분석하는 HIVE 등장 하둡과 하이브는 대용량 데이터를 배치 처리하는데 최적화되어 있지만 실제 업무에서는 데이터를 실시간으로 조회하거나 처리해야 하는 요구사항이 많음 -\u003e 실시간 SQL 질의 분석 기술인 SQL on 하둡 등장\n스쿱 스크립트  데이터를 가져올 데이터베이스 정보 입력 데이터에 대한 SQL 입력 동시에 몇 개의 프로세스를 실행하여 데이터를 가져올지 지정, 적절한 개수 지정해야 함 데이터 베이스의 키 칼럼 입력 가져운 데이터를 저장할 하둡상의 경로 지정  -- connect jbbc:mysql://192.168.10.100:3306/sakila\r-- username Hadoop\r-- password hadoop00\r- query 'select * from city where city like 'k%' '\r# k로 시작하는 행에 대한 모든 열을 출력\r- m2\r- split-by city_id\r- target-dir/user/hadoop\rSQL on 하둡  아파치 드릴 : 맵알이 주축인 프로젝트, 드레멜의 아키텍처와 기능을 동일하게 구현한 오픈 소스 버전 드레멜 아파치 스팅거 : 호튼웍스에서 개발, 기존의 하이브 코드 최대한 이용 임팔라 : 클라우데라에서 개발 주도 샤크 : 인메모리 기반의 대용량 데이터웨어하우스 시스템, 하이브와 호환 아파치 타조 : 고려대 대학원에서 시작, 그루터가 합류해 개발 진행 호크 : EWC에서 분사한 피보탈에서 개발, 상용과 커뮤니티 2가지 버전 프레스토 : 페북에서 개발, 데이터웨어 하우징 엔진, 아파치 라이선스로 공개됨  ","description":"","tags":null,"title":"ADP 공부하기 6","uri":"/posts/certificate/2020-01-28-adp_study6/"},{"categories":["Certificate"],"content":"데이터 처리 프로세스 ETL  데이터의 이동 및 변환 절차와 관련된 용어 데이터 스토어, 웨어하우스, 마트 등에 데이터를 적재 데이터 통합, 이동, 마스터 데이터 관리(MDM)에 활용, 이동과 변환이 목적 대용량 데이터 처리(MPP) 다수 시스템 간 대용량 데이터 교환 Batch, ETL, Real Time등으로 구분  ETL 기능 Extraction : 데이터 소스로부터 데이터 획득 Transformation : 데이터 클렌징, 변한, 형식 변환, 표준화, 통합 등의 비즈니스 룰 Loading : 변형이 완료된 데이터를 특정 시스템에 적재\nETL 작업 단계    step 0\ninterface 다양한 이기종 dbms 및 스프레드시트 등 데이터 소스로부터 데이터를 획득하기 위한 인터페이스 메커니즘 구현     step 1\nStaging ETL 일정에 따라 소스로부터 트랜잭션 데이터 획득 작업 후 획득된 데이터를 스테이징 테이블에 저장   step 2\nProfiling ETL 스테이징 테이블에서 데이터 특성을 식별하고 품질 측정   step 3\nCleansing ETL 다양한 규칙들을 활용해 프로파일링된 데이터의 보정 작업 수행   step 4\nIntegraion ETL 이름, 값, 구조 데이터 충돌을 해소하고 클렌징된 데이터를 통합   step 5\nDemoralizing ETL 운영 보고서 생성, 데이터 웨어하우스, 마트에 대한 데이터 적재를 위해 비정규화 수행     ODS 구성  ODS는 데이터에 대한 추가 작업을 위해 다양한 데이터 소스로부터 데이터를 추출, 통합한 데이터베이스 ODS 내 데이터는 타 정보 시스템이나 데이터 웨어하우스로 이관 ODS를 위한 데이터 통합은 데이터 클렌징, 중복제거, 비즈니스 룰 대비 데이터 무결성 점검의 작업이 포함 일반적으로 Real Time, Near Real Time 트랜잭션 데이터 혹은 가격 등의 원자성을 지닌 하위 수준 데이터를 저장하기 위해 설계  ODS구성 단계  인터페이스 단계   데이터를 획득하는 단계 프로토콜 : OLEDB, ODBC, FTP 등이 사용 데이터 웨어하우스에 대한 RT, NRT OLAP질의를 지원하기 위해 실시간 데이터 복제 인터페이스 기술들이 함께 활용  OLAP : 데이터 웨어하우스 상의 데이터에 대해 다양한 방식으로 다차원 분석 진행\n데이터 스테이징 단계   데이터 소스로부터 트랜잭션 데이터들이 추출되어 하나 또는 그 이상의 스테이징 테이블들에 저장 정규화가 배제되고 스키마는 소스의 구조에 의존적이다. 소스와 스테이징 테이블과의 데이터 매핑은 일대다, 일대일 구조 적재 타임스탬프, 데이터 값에 대한 체크 섬 등의 통제 정보가 추가 다양한 데이터 소스로부터 데이터를 획득해 스테이징 테이블에 적재, 이 때 일괄(BATCH)작업 형태인 정기적 ETL과 실시간 ETL을 혼용할 수 있음.  데이터 프로파일링 단계   데이터 품질 점검의 단계 스테이징 테이블 데이터에 대한 프로파일링 수행 -\u003e 결과 통계 처리 -\u003e 품질 보고서 생성 및 공유  데이터 클렌징 단계   프로파일링 단계에서 식별된 오류 데이터 수정 단계 클렌징 스토어드 프로시저 실행 -\u003e 클렌징 ETL 도구 실행  데이터 인티그레이션 단계   수정 완료한 데이터를 ODS 내의 단일 통합 테이블에 적재 통합 스토어드 프로시저 실행 -\u003e 통합 ETL 도구 실행  익스포트 단계   익스포트 ETL기능을 수행해 익스포트 테이블 생성 다양한 DBMS 클라이언트, 데이터 마트, 웨어하우스에 익스포트 테이블 적재 해당 데이터는 OLAP 비정형 질의에 활용될 수 있음.   데이터 웨어하우스 ODS를 통해 정제, 통합된 데이터가 분석과 보고서 생성을 위해 적재되는 데이터 저장소\n특징\n   주제 중심성\nSubject Oriented 데이터 웨어하우스의 데이터는 실 업무 상황의 특정 이벤트나 업무 항목을 기준으로 구조화 되므로, 최종사용자도 이해하기 쉬운 형태     영속성, 비휘발성\nNON Volatile 최초 저장 이후에는 읽기 전용의 속성, 삭제되지 않는다.   통합성\nIntegrated 기관,조직이 보유한 대부분의 운영 시스템에 의해 생성된 데이터들의 통합본임   시계열성\nTime Variant 운영 시스템은 최신 데이터르 보유하지만 데이터 웨어하우스는 시간 순에 의한 이력 데이터 보유    데이터 웨어하우스의 테이블 모델링 기법  스타 스키마    조인 스키마, 가장 단순\n  단일 사실 테이블을 중심으로 다수의 차원 테이블로 이루어짐\n  전통적인 관계형 데이터베이스를 통해 다차원 데이터베이스 기능 구현\n  사실 테이블은 보통 제 3정규형으로 모델링, 차원 테이블들은 보통 비정규화된 제 2정규형으로 모델링\n  장점 : 스노우 플레이크에 비해 이해하기 쉽고, 쿼리 작성 용이, 조인 테이블 수 적음\n  단점 : 차원 테이블들의 비정규화에 따른 데이터 중복으로 데이터를 적재할 때 많은 시간 소요\n  스노우 플레이크 스키마   차원 테이블을 제 3정규형으로 단점 : 조인 테이블 개수 증가, 쿼리 작성 난이도 상승  ODS DW 비교    구분 ODS DW     데이터 내용 현재, 최신 데이터 오래된 상세 데이터, 현재 상세 데이터, 요약, 2차로 가공된 고도로 요약된 데이터   데이터 양 비교적 소규모 데이터 대규모 데이터   데이터 갱신 지속적 갱신, 현재DB 반영 데이터 축적 보관   기술적 요소 처리의 모든 기능을 사용하도록 설계 단순한 적재와 접근 중심    \nCDC  DB내 데이터 변경을 식별해 필요한 후속처리를 자동화 RT, NRT 데이터 통합을 기반 스토리지 하드웨어 계층에서부터 애플리케이션 계층에 이르기까지 다양한 계층에서 다양한 기술 단일 정보 내 다수의 CDC 매커니즘  CDC 구현 기법  Time Stamp on Rows   테이블 내 마지막 변경 시점을 기록하는 타임스탬프 칼럼을 두고 마지막 변경 타임스탬프 값보다 더 최근의 타임스탬프 값을 갖는 레코드를 변경된 것으로 식별  Version Numbers on Rows   레코드의 버전을 기록하는 칼럼을 두고 더 높은 버전을 보유한 레코드를 변경된 거승로 식별  Status on Rows   타임스탬프 및 넘버 기법에 대한 보완, 변경의 여부를 True,False 로 변경 여부 판단  Time/Version/Status   위의 세가지 특성 모두 활용, 정교한 쿼리 생성에 활용  Triggers on Tables   트리커를 활용해 사전에 등록된 다수 대상 시스템에 변경 데이터를 배포하는 형태 관리 복잡도 증가, 관리 어려움, 확장성 감소 유발 등 시스템 유지보수성을 저하 -\u003e 사용에 주의  Event Programming   변경 식별 기능을 애플리케이션에 구현, 애플리케이션 개발 부담, 복잡도 증가  Log Scanner on Database   DBMS에서 제공하는 트랜잭션 로그에 대한 스캐닝 및 변경에 대한 해석을 통해 구현 작업 규모가 증가될 수 있음 장점 : db와 애플리케이션 영향도 최소화, 식별 지연시간, 트랜잭션 무결성 영향도 최소화, 스키마 변경 불필요  CDC 구현 방식 push : 소스에서 변경을 식별하고 대상 시스템에 대한 변경 데이터를 적재 pull : 대상 시스템에서 데이터 소스를 정기적으로 살피고 필요시 데이터 다운\n\nEAI 비즈니스 프로세스를 중심으로 기업 내 각종 애플리케이션 간의 상호연동이 가능하도록 함\n 데이터 연계 -\u003e 상호 융화 내지 동기화 애플리케이션을 프로세스 및 메시지 차원에서 통합 관리 EAI를 통해 비즈니스 프로세스를 자동화하고 실시간으로 통합 연계 RT, NRT 처리 중심  데이터 연계 방식 기존 : POINT TO POINT\n 정보 시스템 개발 시 정보 시스템들 간의 데이터를 연계, 복잡함 데이터 통합과 표준화 불가능, 유지 보수성 저하, 관리 비용 상승 N(N-1)/2개의 연결  EAI의 연계방식 : Hub and Spoke\n 가운데에 허브 역할을 하는 브로커를 두고 연결 노드들의 데이터 연계 요구를 중계함으로써 구조를 단순화 각 연결의 대상이 되는 노드들은 Spoke  EAI 구성 요소 어댑터 : 각 정보 시스템과 EAI허브 간의 연결성 확보 버스 : 어댑터를 매개로 연결된 각 정보 시스템들 간의 데이터 연동 경로 브로커 : 데이터 연동 규칙 통제 트랜스포머 : 데이터 형식 변환을 담당\nEAI 구현 유형  Mediation   EAI 엔진이 중개자, 특정 정보 시스템 내의 데이터 생성, 갱신, 신규 트랜잭션 완료 등 이벤트 발생을 식별, 미리 약속된 정보 시스템에 해당 내용(데이터) 전달 Publish/subscribe Model  Federation   EAI 엔진이 외부 정보 시스템으로부터 데이터 요청들을 일괄적으로 수령해 전달 Request/reply Model  EAI 활용 효과  정보 시스템 개발 및 유지 보수비용 절감 기업 정보 시스템의 지속적 발전 기반 확보 협력사, 고객, 파트너와의 상호 협력 프로세스 연계 웹 서비스, 인터넷 비즈니스를 위한 기본 토대 확립 데이터 동기화, 데이터 표준화 기반 제공  EAI, ESB    구분 EAI ESB     기능 Hub를 이용해 비즈니스 로직을 중심으로 애플리케이션을 통합, 연계 BUS를 이용해 서비스 중심으로 시스템을 유기적으로 연계   통합 관점 애플리케이션 프로세스   로직연동 개별 애플리케이션에서 수행 ESB에서 수행   아키텍처 단일 접점인 허브시스템을 이용한 중앙집중식 연결구조 버스 형태의 느슨하고 유연한 연결구조    ","description":"","tags":null,"title":"ADP 공부하기 5","uri":"/posts/certificate/2020-01-27-adp_study5/"},{"categories":["Certificate"],"content":"비정형 데이터마이닝 텍스트 마이닝  입력된 텍스트를 구조화해 그 데이터에서 패턴을 도출 후, 결과를 평가 및 해석 다양한 포맷의 문서로부터 텍스트를 추출 자연어로 구성된 비정형 텍스트 데이터 속에서 정보나 관계를 발견  텍스트마이닝 기능 : 문서 요약, 분류, 군집, 특성 추출\nCorpus  데이터의 정제 통합 선택 변환의 과정을 거친 구조화된 단계 ‘tm’패키지에서 문서를 관리하는 기본 구조, 문서들의 집합  tm패키지 함수  VCorpus() : 문서를 Corpus class로 만들어줌. 결과는 r메모리에만 PCorpus() : 문서를 Corpus class로 만들어 R 외부 db나 파일로 관리 DirSource(), VectorSource(), DataframeSource() : 디렉토리, 벡터, 데이터 프레임으로부터 코퍼스 생성을 위한 소스를 만들어 줌 tm_map(x,FUN) : x데이터에 대해 FUN을 적용  FUN에 들어가는 함수\nas.PlainTextDocument : XML문서를 text로 전환 stripWhitespace : space 제거 removewords, stopwords(“english”) : 띄어쓰기, 시제 표준화 DocumentTermMatrix : 코퍼스로부터 문서별 특정 문자 빈도표 TermDocumentMatrix : 코퍼스로부터 단어별 문서의 빈도표\nTerm-Document Matrix 문서를 plain text로 전환, 공백 제거, lowercase변환, 불용어(stopword)처리, 어간추출(stemming) 등의 작업을 수행하고 문서번호와 단어 간 사용여부 또는 빈도수를 이용해 matrix만듬\nDirectory 텍스트마이닝 분석 시 사용하고자 하는 단어들의 집합\n감성분석 문장에서 사용된 단어의 긍정과 부정 여부에 따라 전체 문장의 긍정/부정 판별\n한글처리 KoNLP 등 사용, rJava패키지, JRE프로그램 설치해야함 명사를 추출할 때는 extractNoun(“문장”) 함수\n워드 클라우드 단어들을 크기, 색 등으로 나타내어 구름 등과 같은 형태\n ## 사회연결망 분석\rSNA  개인과 집단들 간의 관계를 노드,링크로 모델링 제이콥 마리노(개념), 바르네스(1954에 처음)  SNA 분류  집합론적 방법  객체들 간의 관계를 관계 쌍으로 표현   그래프 이론을 이용한 방법  객체를 점으로 표현, 연결은 선으로 표현   행렬  관계가 존재하면 1, 그렇지 않으면 0 행과 열이 같은 개체가 배열(1원), 다른 개체(2원) 준연결망 : 고객-상품 행렬에서 사람들 사이에 상호작용이 없어도 관계를 인위적으로 설정 고객 트랜잭션(고객이 동일한 상품을 1개 이상 구매하면 직접적인 상호작용이 있다고 표현) 상품을 동시에 구매 -\u003e 서로 상호관계에 있음     \r   연결 정도 중심성 - 한점에 직접적으로 연결된 점들의 합- 한점에 얼마나 많은 다른 점들이 관계를 맺고 있는지를 기준으로 중심에 위치하는 정도를 계량화-연결된 노드 수가 많을 수록 연결정도 중심성이 높아짐     근접 중심성 - 한점에 직접적으로 연결된 점들의 합- 근접 중심성이 높을 수록 네트워크의 중앙에 위치   매개 중심성 - 네트워크 내 한 점이 담당하는 매개자 혹은 중개자 역할- 한 노드가 연결망 내의 다른 노드 사이의 최다 연결 경로 위에 위치하면 할수록 그 노드의 매개 중심성이 높음   위세 중심성 - 자신의 연결정도 중심성으로부터 발생하는 영향력과 자신과 연결된 타인의 영향력을 합함- 위세가 높은 노드들과 관계가 많을수록 자신의 위세도 높아짐- 보나시치 권력지수 : 연결된 노드의 중요성에 가중치를 둬 노드의 중심성 측정     \r### SNA적용\r분석용 솔루션 : KXEN, SAS, XARACT,Indiro, Onalytica, Unicet, Inflow, Pagek 등\rMapReduce(분산 처리 기술)을 활용, Giraph(하둡 기반 그래프 프로세싱 프레임워크)\rRHadoop, RHIPE : R과 하둡 연동\rSNA단계  그래프 생성 그래프를 목적에 따라 가공, 분석 커뮤니티를 탐지하고 각 노드의 역할을 정의해 어떠한 ROLE로 다른 객체들에게 영향력을 더 효율적으로 줄 수 있는지를 정의 위 결과를 데이터화하여 다른 데이터마이닝 기법과 연계하는 단계   데이터화는 SNA를 통해 얻어진 커뮤니티의 프로파일을 해당 그룹의 연령,성별 등과 같은 고객 프로파일 평균값으로 산출해 그룹에 속할 개별 고객 속성에 그룹 넘버와 ROLE을 결합해 추가하는 단계임  R에서의 SNA 네트워크 레벨 통계량 degree, shortest paths, reachability, density, reciprocity, transitivity, triad census\n커뮤니티 수를 측정하는 방법 WALKRAP알고리즘\n 일련의 random walk과정을 통해 커뮤니티를 발견 각 버텍스(그래프 꼭지점)를 하나의 커뮤니티로 취급해 점차 더 큰 그룹을 병합하면서 클러스터링 코드를 실행하면 군집화 개수와 그래프 결과가 나타남  1 2  friend_comm = walktrap.community(m182,step=200,modularity=TRUE) dend = as.dendrogram(friend_comm,use.modularity=TRUE)   Edge Betweenness method\n 그래프에 존재하는 최단거리 중 몇 개가 그 edge를 거쳐가는 지를 이용해 edge-betweenness점수 측정 높은 edge-betweenness점수를 갖는 edge가 클러스터를 분리하는 속성을 가짐  1 2  edge.betweenness.community(m182) plot(as.dendrogram(friend_comm))   활용방안 몇 개의 집단으로 구성되고 집단 간 특성은 무엇이고 해당 집단에서 영향력 있는 고객은 누구이고 시간의 흐름과 고객 상태의 변화에 따라 다음에 누가 영향을 받을지를 기반으로 fraud, churn/acquisition prediction, product recommendation 등에 활용\n","description":"","tags":null,"title":"ADP 공부하기 4","uri":"/posts/certificate/2020-01-25-adp_study4/"},{"categories":["Certificate"],"content":"정형 데이터마이닝 1. 데이터 마이닝 개요 변수 선택 filter method\n 각각의 변수들에 대해 통계적 점수 부여, 점수로 순위를 매김 chi squared, information gain, correlatioin 등  wrapper method\n 변수 간 상호 작용 감지, 변수의 일부만 모델링에 사용 후 결과 평가 -\u003e 반복 recursive feature elimination algorithm  embedded method\n filter method와 wrapper method 결합, 과적합을 줄이기 위해 내부적 규제 ridge, lasso, elastic net wrapper는 학습을 마친 후 비교, embedded는 학습 과정에서 최적화된 변수 선택  머신러닝  지도학습, 비지도학습, 강화학습 지도학습 : knn, 선형회귀, svm, 의사결정 나무, 신경망 비지도학습 : 군집분석, pca, 연관분석, 사회연결망 분석, 텍스트 마이닝  딥러닝 DNN : 인공신경망(ANN)에서 은닉층이 여러개 - 암 진단 시스템, 주가지수예측, 환율예측, 기업신용평가 CNN : 다계층 퍼셉트론, 여러 개의 합성곱 계층과 인공 신경망 계층으로 이루어짐 - 자율 주행 자동차, 멀티미디어 식별 RNN : 시간의 흐름에 따른 따른 데이터 학습, 기준 시점과 다음 시점의 네트워크 연결 - 음성 인식, 번역, 의미 판단, 이미지 캡션 생성, 자연어 처리\n딥러닝 지원 라이브러리 파이썬 theano\n keras : 오픈 소스 신경망 라이브러리, dnn의 빠른 실험 lasagne : theano의 복잡성을 추상화하고 보다 편리한 인터페이스  chainer : “define-by-run\"모델을 기반으로 NLP에서 많이 이용 TENSORFLOW : 구글에서 만든 오픈소스 패키지, 플로우 그래프 CXXNET : MShadow 라이브러리, 멀티GPU지원\nC++  caffe : 이미지 분석에 특화 mxnet : 파이썬도 지원, 대규모 데이터셋에 효과적, 아마존 웹 서비스에서 딥러닝 프레임워크 지원  JAVA deepLearning4j : 비즈니스용 딥러닝 플랫폼\nR darch, deepnet 2. 분류 분석 나이브 베이즈 $$posterior = \\frac{prior \\times likelihood}{evidence}$$\n 문서를 여러 범주(스팸,스포츠) 중 하나로 판단 다른 속성들이 독립적 : 클래스 조건 독립성  K-NN 유클리디안(대표적), 맨하탄, 민코우스키 거리 사용 유클리디안 : 두 점 거리 제곱합의 제곱근 맨하탄 : 절대값\n 장점 : 사용 간단, 기준을 몰라도 데이터 분류 가능, 데이터 처리가 용이 단점 : k값 결정이 어렵, 비수치 데이터일 경우 유사도 정의 어렵, 이상치가 있으면 큰 영향을 받음  SVM 새로운 데이터가 어떤 범주에 속하는지 판단하는 비확률적 이진 선형 분류모델을 생성\n 가장 큰 폭을 가진 경계를 찾음  초평면 : 각 그룹을 구분하는 분류자 서포트 벡터 : 초평면에 가장 가까이에 붙어있는 최전방 데이터 마진 : 포여면과 서포트 벡터 사이의 수직거리\n마진을 최대화하는 초평면(MMH)을 찾아 수행\n 비선형 분류에서는 커널 트릭 사용 장점 : 분류, 예측 모두 가능, 과적합 정도 적음, 정확도가 높다, 저차원 고차원 상관없이 잘 작동 단점 : 전처리와 매개변수 설정에 따라 정확도가 달라짐, 해석이 어려움, 속도가 느리고, 할당량이 크다.   군집분석 resampling k-fold cross validation k-1개 집단으로 학습, 1개로 성능 테스트 -\u003e k번 반복-\u003e mse평균 bootstrap 표본에 대해 다시 재표본을 여러 번 추출, 단순랜덤 복원추출법 사용\n 63.2%만 선택, 나머지 OOB 데이터(OOB-error: 실제값과 예측값 사이 오차)  군집화 기법 밀도기반 군집분석 어느 점을 기준으로 반경 내에 최소 개수만큼의 데이터들은 가질 수 있도록 밀도에 의해 군집을 형성\n DBSCAN : 밀도 한계점에 따라 군집 형성(대표적) OPTICS : 군집화 구조 식별을 위해 부가적 순서를 생성 DENCLUE : 밀도 분포함수에 기초  격자기반 데이터가 존재하는 공간을 격자구조로 이루어진 유한개의 셀들로 양자화하여 셀을 이용해 군집화, 셀의 수에만 의존\n STING : 결자 셀에 저장된 통계정보를 탐색 Wavecluster : Wavelet변환 기법 사용 CLIQUE : 고차원 데이터 공간의 군집화를 위한 격자 및 밀도기반  군집 분석의 타당성 지표 Silhouette(실루엣)\n 군집 내의 응집도와 군집 간 분리도를 이용한 지표 군집 내 거리가 짧고 군집 간 거리가 멀수록 값이 커짐  dunn index\n 군집 간 거리는 멀고, 군집 내 분산은 작을수록 군집화가 잘 이루어짐 dunn index가 클수록 군집이 잘 형성됨  BMU  SOM에서는 각 학습 단계마다 입력층으로부터 하나의 표본 벡터를 임의로 선택하고 경쟁층의 프로토타입 벡터와의 거리 계산 그 후 표본 벡터와 거리가 가장 가까운 프로토타입 벡터ㄹ르 선택 BMU는 선택된 프로토타입 벡터를 나타내는 용어  ","description":"","tags":null,"title":"ADP 공부하기 3","uri":"/posts/certificate/2020-01-24-adp_study3/"},{"categories":["Certificate"],"content":"내가 공부한 것을 요약하는 위주이기 때문에 아는 내용은 가볍게 넘어감.\n데이터 분석 1. 통계분석 연속형 확률분포  t분포 평균의 동일성 검정, 데이터가 연속형일때, 자유도 30미만 카이제곱 분포 두 집단의 동질성 검정,자유도 (r-1)(c-1) F분포 등분산성 검정, 자유도가 두 개고 커질수록 정규분포에 가까움  r을 활용한 one t-검정 t검정은 모두 모집단이 정규성을 만족한다고 가정\n1 2  shapiro.test(data) t.test(data, alternaive=\"two.sided\",mu=200)   p-value가 0.05보다 높으면 정규분포를 따르는 것임 -\u003e t-test 수행 가능\npaired sample t-test(대응표본) 한 모집단에 대해 두 가지 처리를 했을 때 두 가지의 평균의 차이를 검정 개체별로 짝지어진 관측값 사의 차이로 검정\n1  t.test(data$before, data$after , alternaive=\"less\",paired=\"True\")   m 변수가 따로 있지만 차이가 0인지 검정하기 때문에 따로 필요없음\npaired sample t-test(독립표본) 두 개의 독립된 모집단 검정, 모분산이 동일해야(등분산 검정 선행)\n1 2  var.test(formula , data , alternaive=\"two.sided\") t.test(data$before, data$after , alternaive=\"less\",var.equal=\"True\")    분산분석 one-way anova   하나의 범주형 변수의 영향을 알아보기 위함\n  표본의 수가 같지 않아도 되고 모집단의 수는 제한이 없다.\n  정규성, 등분산성 가정\n  사후 검정 분산분석의 결과 기각되어 평균의 차이가 있음이 통계적으로 증명되었을 경우, 어떤 집단들에 대해 평균의 차이가 존재하는지 알아보기 위함 Duncan의 Multiple Range Test(MRT), Fisher의 최소유의차(LSD), Tukey의 HSD, Scheffe의 방법이 있다.\n  r에서의 일원배치 분산분석  그룹을 구분하는 기준이 되는 변수는 반드시 factor형이어야 함.  1 2 3  result = aov(formula, data) summary(result) TukeyHSD(result,conf.level=0.95)   diff a-b로 연결되어 있을 때 양수면 a가 유의하게 큰 값을 가짐\ntwo-way anova  두 개의 범주형 변수의 영향을 알아봄 교호작용에 대한 검증이 진행되어야 함 모형 $$y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}$$     요인 제곱합 자유도 평균제곱합 F     요인a $$SS_a$$ $$I-1$$ $$MS_a = \\frac{SSA}{I-1}$$ $$F_a = \\frac{MSA}{MSE}$$   요인b $$SS_b$$ $$J-1$$ $$MS_b = \\frac{SSB}{J-1}$$ $$F_b = \\frac{MSB}{MSE}$$   상호 작용 $$SS_{a \\times b}$$ $$(I-1)(J-1)$$ $$MS_{ab} = \\frac{SSAB}{(I-1)(J-1)}$$ $$F_{ab} = \\frac{MSAB}{MSE}$$   오차 $$SSE$$ $$IJ(n-1)$$ $$MSE = \\frac{SSA}{IJ(n-1)}$$    전체 $$SST$$ $$IJn-1$$      귀무가설(H0)\n 변수에 따른 종속 변수의 값에는 차이가 없다. A, B변수의 상호작용 효과가 없다. 대립가설(H1) 변수에 따른 종속 변수의 값에는 차이가 있다.(a가 모두 0이라 할 수 없다) a와 b변수의 상호작용 효과가 있다.  교호작용 두 가지 이상의 특정 변수 조합에서 일어나는 효과 (상관관계가 존재할 경우 교호작용이 있다는 의미)\n 교호작용이 있다면 검정이 무의미하다   실험계획법  개념 시스템이나 프로세스의 결과에 영향을 미치는 인자를 도출, 측정 데이터를 실험적으로 설계 최소 실험 횟수로 최대의 정보를 얻는 것 목적 분산분석 및 검정과 추정 : 유의미한 영향, 요인의 영향 파악 최적 반응 조건의 결정 : 어떤 인자를 사용해야 가장 원하는 결과값을 얻을지 파악 오차항 추정의 문제 : 이해하기 어렵던 오차와 그 변동에 관한 정도  실험계획법의 원리  랜덤화의 원리, 반복의 원리, 블록화의 원리, 직교화, 교락 교락 : 2개 이상의 효과를 구별할 수 없도록 계획적으로 조합 블록 : 실험 단위가 균일할 수 있도록 단위를 모은 것 반복 : 인자들의 동일한 수준 조합에서 다회의 실험을 진행  실험계획법의 종류  요인배치법  모든 인자간의 수준 조합에서 실험이 이루어지는 완전랜덤화방법 교호효과를 포함한 모든 요인효과를 추정할 수 있다. $K^n$형 요인실험 : 인자 수가 n이고, 수준 수가 k인 실험계획법   분할법  몇 단계로 분할하여 각 단계별로 완전 랜덤하게 실험 순서를 결정 랜덤화 어려운 것을 1차 단위, 쉬운 것을 후 단위로 배치   교락법  검출할 필요가 없는 교호작용을 다른 요인과 교락하도록 배치하는 방법 실험 전체를 몇 개의 블록으로 나누어 배치, 실험 횟수를 줄일 수 있다. 교호작용을 교락시키기 때문에 주효과가 높게 추정   난괴법  실험 단위를 몇 개의 반복으로 나누어 배치 a가 모수인자. b가 변량인자일 때, a의 수준수가 1, b의 수준수가 m인 반복이 없는 이원배치 분산분석방법이다.     교차분석 범주형 자료인 두 변수 간의 관계\n 적합도 검정  1  chisq.test(data,p=c(0.2,0.8))    독립성 검정 동질성 검정 독립성 검정과 같은 방법으로 진행, 가설만 다름   중심 극한 정리 n이 커질수록(30이상) 표본평균의 분포가 정규분포에 가까워짐\n","description":"","tags":null,"title":"ADP 공부하기 1","uri":"/posts/certificate/2020-01-23-adp_study_1/"},{"categories":["Certificate"],"content":"2. 회귀분석 정규화 선형회귀 선형회귀 계수에 대한 제약조건 추가, 과적합을 막음 계수의 크기를 제한하는 방법으로 제약조건 추가\n  Ridge Regression\n 가중치의 제곱합을 최소화 모든 원소가 0에 가까워짐, L2 규제    Lasso Regression\n 가중치 절대값의 합을 최소화 라쏘에서는 릿지와 다르게 가중치가 0이 되게 함, L1 규제    Elastic Net\n 릿지와 라쏘를 절충 두 개의 모수     GLM 종속변수를 적절한 함수로 변화시켜 독립변수를 선형 결합으로 모형화\n 랜덤성분(반응변수), 체계적 성분(선형식), 연결함수(랜덤과 체계적 연결)     랜덤성분 연결함수 체계적 성분 model     Normal identity(항등) 연속형 regression   Normal identity(항등) 범주형 ANOVA   Normal identity(항등) Mixed regression with Indicator Anova   Binomial Logit Mixed Logistic regression   Poisson Log Mixed log-linear   Multinomial Generalized Logit Mixed Multinomial response     회귀분석의 영향력 진단 적합된 회귀모형의 안전성을 평가, 많은 변동이 있다면 안정성이 약함\n 회귀직선의 기울기에 영향을 크게 주는 점을 영향점이라고 함  cook’s distance full model에서 i번째 관측치를 포함해 계산한 적합치와 i번째 관측치를 포함하지 않고 계산한 적합치 사이 거리 DFBETAS 이 값이 커지면 i번째 관측치가 영향치 혹은 이상치일 가능성이 높다. DFFITS i번째 관측치 제외 시 종속변수 예측치의 변화정도를 측정 Leverage H 관측치가 다른 관측치 집단으로부터 떨어진 정도\n 2 * (p+1)/n 보다 크면 영향치이거나 이상치라고 봄 $$H = X(X^`X)^{-1}X$$   더빈 왓슨  오차항이 독립성을 만족하는지를 검정 2에 가까울수록 오차항의 자기상관이 없음을 의미 0에 가까울수록 양의 상관관계가 있고 4에 가까을수록 음의 상관관계 -» 상관관계가 있어 회귀식이 부적합함을 의미  변수 선택의 기준으로 사용되는 통계량 수정된 결정계수 : 결정계수의 단점 보완 Mallow’s Cp\n 최소자승법을 사용해 회귀모형의 적합성 평가 일반적으로 cp값이 작고 p+상수(변수개수+상수)에 가까운 모형을 선택     CP값 해석     P(변수의 개수)와 비슷한 경우 bias가 작고 우수한 모델   P(변수의 개수)보다 큰 경우 bias가 크고 추가적인 변수가 필요한 모델   P(변수의 개수)보다 작은 경우 variance의 증가폭보다 bias의 감소폭이 더 크고 필요 없는 변수가 존재하는 모델     변수변환 정규성, 선형성, 등분산성을 만족하지 못하는 경우 변수를 변환함으로써 교정\n 로그, 지수 변환 더미변수 생성 box-cox 변환  정규성을 만족하도록 반응 변수를 다음과 같이 변환 $$ g_\\lambda(y) = \\begin{cases} y^\\lambda, \u0026 \\lambda \\ne 0 \\\nlogy, \u0026 \\lambda = 0 \\end{cases} $$ $\\lambda$는 우도함수를 최대화 시키는 조건으로 계산    ","description":"","tags":null,"title":"ADP 공부하기 2","uri":"/posts/certificate/2020-01-23-adp_study_2/"}]
